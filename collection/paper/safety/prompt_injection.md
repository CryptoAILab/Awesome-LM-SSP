# A7. Prompt Injection
- [2025/10] **[PromptLocate: Localizing Prompt Injection Attacks](https://arxiv.org/abs/2510.12252)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![S&P'26](https://img.shields.io/badge/S&P'26-f1b800)
- [2025/10] **[In-Browser LLM-Guided Fuzzing for Real-Time Prompt Injection Testing in Agentic AI Browsers](https://arxiv.org/abs/2510.13543)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/10] **[PIShield: Detecting Prompt Injection Attacks via Intrinsic LLM Features](https://arxiv.org/abs/2510.14005)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/10] **[Indirect Prompt Injections: Are Firewalls All You Need, or Stronger Benchmarks?](https://arxiv.org/abs/2510.05244)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/10] **[Backdoor-Powered Prompt Injection Attacks Nullify Defense Methods](https://arxiv.org/abs/2510.03705)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/10] **[AgentTypo: Adaptive Typographic Prompt Injection Attacks against Black-box Multimodal Agents](https://arxiv.org/abs/2510.04257)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/10] **[VortexPIA: Indirect Prompt Injection Attack against LLMs for Efficient Extraction of User Privacy ](https://arxiv.org/abs/2510.04261)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/10] **[Unified Threat Detection and Mitigation Framework (UTDMF): Combating Prompt Injection, Deception, and Bias in Enterprise-Scale Transformers](https://arxiv.org/abs/2510.04528)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/10] **[RL Is a Hammer and LLMs Are Nails: A Simple Reinforcement Learning Recipe for Strong Prompt Injection](https://arxiv.org/abs/2510.04885)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/10] **[WAInjectBench: Benchmarking Prompt Injection Detections for Web Agents ](https://arxiv.org/abs/2510.01354)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/09] **["Your AI, My Shell": Demystifying Prompt Injection Attacks on Agentic AI Coding Editors](https://arxiv.org/abs/2509.22040)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/09] **[EchoLeak: The First Real-World Zero-Click Prompt Injection Exploit in a Production LLM System](https://arxiv.org/abs/2509.10540)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/09] **[Early Approaches to Adversarial Fine-Tuning for Prompt Injection Defense: A 2022 Study of GPT-3 and Contemporary Models](https://arxiv.org/abs/2509.14271)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/09] **[A Multi-Agent LLM Defense Pipeline Against Prompt Injection Attacks](https://arxiv.org/abs/2509.14285)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/09] **[AEGIS : Automated Co-Evolutionary Framework for Guarding Prompt Injections Schema](https://arxiv.org/abs/2509.00088)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/08] **[Cybersecurity AI: Hacking the AI Hackers via Prompt Injection](https://arxiv.org/abs/2508.21669)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/08] **[Publish to Perish: Prompt Injection Attacks on LLM-Assisted Peer Review](https://arxiv.org/abs/2508.20863)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/08] **[PromptSleuth: Detecting Prompt Injection via Semantic Intent Invariance](https://arxiv.org/abs/2508.20890)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/08] **[Prompt-in-Content Attacks: Exploiting Uploaded Inputs to Hijack LLM Behavior](https://arxiv.org/abs/2508.19287)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/08] **[Too Easily Fooled? Prompt Injection Breaks LLMs on Frustratingly Simple Multiple-Choice Questions](https://arxiv.org/abs/2508.13214)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/08] **[IPIGuard: A Novel Tool Dependency Graph-Based Defense Against Indirect Prompt Injection in LLM Agents](https://arxiv.org/abs/2508.15310)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800) ![EMNLP'25](https://img.shields.io/badge/EMNLP'25-f1b800)
- [2025/08] **[Prompt Injection Vulnerability of Consensus Generating Applications in Digital Democracy](https://arxiv.org/abs/2508.04281)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/08] **[PLA: Prompt Learning Attack against Text-to-Image Generative Models](https://arxiv.org/abs/2508.03696)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![ICCV'25](https://img.shields.io/badge/ICCV'25-f1b800)
- [2025/08] **[LeakSealer: A Semisupervised Defense for LLMs Against Prompt Injection and Leakage Attack](https://arxiv.org/abs/2508.00602)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/07] **[Counterfactual Evaluation for Blind Attack Detection in LLM-based Evaluation Systems](https://arxiv.org/abs/2507.23453)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/07] **[Invisible Injections: Exploiting Vision-Language Models Through Steganographic Prompt Embedding](https://arxiv.org/abs/2507.22304)** ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2025/07] **[MAD-Spear: A Conformity-Driven Prompt Injection Attack on Multi-Agent Debate Systems](https://arxiv.org/abs/2507.13038)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/07] **[Prompt Injection 2.0: Hybrid AI Threats](https://arxiv.org/abs/2507.13169)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/07] **[How Not to Detect Prompt Injections with an LLM](https://arxiv.org/abs/2507.05630)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/07] **[May I have your Attention? Breaking Fine-Tuning based Prompt Injection Defenses using Architecture-Aware Attacks](https://arxiv.org/abs/2507.07417)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/07] **[Defending Against Prompt Injection With a Few DefensiveTokens](https://arxiv.org/abs/2507.07974)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2025/07] **[Meta SecAlign: A Secure Foundation LLM Against Prompt Injection Attacks](https://arxiv.org/abs/2507.02735)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/06] **[AgentAuditor: Human-Level Safety and Security Evaluation for LLM Agents](https://arxiv.org/abs/2506.00641)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/06] **[Sentinel: SOTA model to protect against prompt injections](https://arxiv.org/abs/2506.05446)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/06] **[To Protect the LLM Agent Against the Prompt Injection Attack with Polymorphic Prompt](https://arxiv.org/abs/2506.05739)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/06] **[LLMail-Inject: A Dataset from a Realistic Adaptive Prompt Injection Challenge](https://arxiv.org/abs/2506.09956)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Dataset](https://img.shields.io/badge/Dataset-87b800)
- [2025/06] **[Simple Prompt Injection Attacks Can Leak Personal Data Observed by LLM Agents During Task Execution](https://arxiv.org/abs/2506.01055)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/06] **[VPI-Bench: Visual Prompt Injection Attacks for Computer-Use Agents](https://arxiv.org/abs/2506.02456)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/06] **[TracLLM: A Generic Framework for Attributing Long Context LLMs](https://arxiv.org/abs/2506.04202)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![USENIX_Security'25](https://img.shields.io/badge/USENIX_Security'25-f1b800)
- [2025/05] **[A Critical Evaluation of Defenses against Prompt Injection Attacks](https://arxiv.org/abs/2505.18333)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/05] **[Invisible Prompts, Visible Threats: Malicious Font Injection in External Resources for Large Language Models](https://arxiv.org/abs/2505.16957)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/05] **[Is Your Prompt Safe? Investigating Prompt Injection Attacks Against Open-Source LLMs](https://arxiv.org/abs/2505.14368)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/05] **[Lessons from Defending Gemini Against Indirect Prompt Injections](https://arxiv.org/abs/2505.14534)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/05] **[Defending against Indirect Prompt Injection by Instruction Detection](https://arxiv.org/abs/2505.06311)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2025/05] **[OET: Optimization-based prompt injection Evaluation Toolkit](https://arxiv.org/abs/2505.00843)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/04] **[CachePrune: Neural-Based Attribution Defense Against Indirect Prompt Injection Attacks](https://arxiv.org/abs/2504.21228)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2025/04] **[Robustness via Referencing: Defending against Prompt Injection Attacks by Referencing the Executed Instruction](https://arxiv.org/abs/2504.20472)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/04] **[Token-Efficient Prompt Injection Attack: Provoking Cessation in LLM Reasoning via Adaptive Token Compression](https://arxiv.org/abs/2504.20493)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/04] **[Adversarial Attacks on LLM-as-a-Judge Systems: Insights from Prompt Injections](https://arxiv.org/abs/2504.18333)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/04] **[Breaking the Prompt Wall (I): A Real-World Case Study of Attacking ChatGPT via Lightweight Prompt Injection](https://arxiv.org/abs/2504.16125)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/04] **[DataSentinel: A Game-Theoretic Detection of Prompt Injection Attacks](https://arxiv.org/abs/2504.11358)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/04] **[StruPhantom: Evolutionary Injection Attacks on Black-Box Tabular Agents Powered by Large Language Models](https://arxiv.org/abs/2504.09841)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/04] **[Separator Injection Attack: Uncovering Dialogue Biases in Large Language Models Caused by Role Separators](https://arxiv.org/abs/2504.05689)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/03] **[Encrypted Prompt: Securing LLM Applications Against Unauthorized Actions](https://arxiv.org/abs/2503.23250)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/03] **[Defeating Prompt Injections by Design](https://arxiv.org/abs/2503.18813)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/03] **[Adaptive Attacks Break Defenses Against Indirect Prompt Injection Attacks on LLM Agents](https://arxiv.org/abs/2503.00061)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/02] **[Prompt Inject Detection with Generative Explanation as an Investigative Tool](https://arxiv.org/abs/2502.11006)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2025/02] **[RTBAS: Defending LLM Agents Against Prompt Injection and Privacy Leakage](https://arxiv.org/abs/2502.08966)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2025/02] **[MELON: Indirect Prompt Injection Defense via Masked Re-execution and Tool Comparison](https://arxiv.org/abs/2502.05174)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2025/01] **[PromptShield: Deployable Detection for Prompt Injection Attacks](https://arxiv.org/abs/2501.15145)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2025/01] **[Computing Optimization-Based Prompt Injections Against Closed-Weights Models By Misusing a Fine-Tuning API](https://arxiv.org/abs/2501.09798)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/12] **[The Task Shield: Enforcing Task Alignment to Defend Against Indirect Prompt Injection in LLM Agents](https://arxiv.org/abs/2412.16682)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2024/12] **[Towards Action Hijacking of Large Language Model-based Agent](https://arxiv.org/abs/2412.10807)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Agent](https://img.shields.io/badge/Agent-87b800)
- [2024/12] **[Trust No AI: Prompt Injection Along The CIA Security Triad](https://arxiv.org/abs/2412.06090)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/11] **[Universal and Context-Independent Triggers for Precise Control of LLM Outputs](https://arxiv.org/abs/2411.14738)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2024/11] **[Attention Tracker: Detecting Prompt Injection Attacks in LLMs](https://arxiv.org/abs/2411.00348)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2024/11] **[Defense Against Prompt Injection Attack by Leveraging Attack Techniques](https://arxiv.org/abs/2411.00459)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2024/10] **[Systematically Analyzing Prompt Injection Vulnerabilities in Diverse LLM Architectures](https://arxiv.org/abs/2410.23308)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/10] **[InjecGuard: Benchmarking and Mitigating Over-defense in Prompt Injection Guardrail Models](https://arxiv.org/abs/2410.22770)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/10] **[HijackRAG: Hijacking Attacks against Retrieval-Augmented Large Language Models](https://arxiv.org/abs/2410.22832)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2024/10] **[FATH: Authentication-based Test-time Defense against Indirect Prompt Injection Attacks](https://arxiv.org/abs/2410.21492)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2024/10] **[Embedding-based classifiers can detect prompt injection attacks](https://arxiv.org/abs/2410.22284)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/10] **[Hacking Back the AI-Hacker: Prompt Injection as a Defense Against LLM-driven Cyberattacks](https://arxiv.org/abs/2410.20911)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/10] **[Making LLMs Vulnerable to Prompt Injection via Poisoning Alignment](https://arxiv.org/abs/2410.14827)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/10] **[Backdoored Retrievers for Prompt Injection Attacks on Retrieval Augmented Generation of Large Language Models](https://arxiv.org/abs/2410.14479)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2024/10] **[F2A: An Innovative Approach for Prompt Injection by Utilizing Feign Security Detection Agents](https://arxiv.org/abs/2410.08776)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/10] **[Prompt Infection: LLM-to-LLM Prompt Injection within Multi-Agent Systems](https://arxiv.org/abs/2410.07283)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Agent](https://img.shields.io/badge/Agent-87b800)
- [2024/10] **[Aligning LLMs to Be Robust Against Prompt Injection](https://arxiv.org/abs/2410.05451)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2024/09] **[StruQ: Defending Against Prompt Injection with Structured Queries](https://arxiv.org/html/2402.06363v2)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800) ![USENIX_Security'25](https://img.shields.io/badge/USENIX_Security'25-f1b800)
- [2024/09] **[System-Level Defense against Indirect Prompt Injection Attacks: An Information Flow Control Perspective](https://arxiv.org/abs/2409.19091)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/09] **[GenTel-Safe: A Unified Benchmark and Shielding Framework for Defending Against Prompt Injection Attacks](https://arxiv.org/abs/2409.19521)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/09] **[PROMPTFUZZ: Harnessing Fuzzing Techniques for Robust Testing of Prompt Injection in LLMs](https://arxiv.org/abs/2409.14729)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/09] **[Applying Pre-trained Multilingual BERT in Embeddings for Improved Malicious Prompt Injection Attacks Detection](https://arxiv.org/abs/2409.13331)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2024/08] **[Rag and Roll: An End-to-End Evaluation of Indirect Prompt Manipulations in LLM-based Application Frameworks](https://arxiv.org/abs/2408.05025)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2024/08] **[Empirical Analysis of Large Vision-Language Models against Goal Hijacking via Visual Prompt Injection](https://arxiv.org/abs/2408.03554)** ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2024/07] **[Prompt Injection Attacks on Large Language Models in Oncology](https://arxiv.org/abs/2407.18981)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/06] **[Dataset and Lessons Learned from the 2024 SaTML LLM Capture-the-Flag Competition](https://arxiv.org/abs/2406.07954)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/06] **[Adversarial Search Engine Optimization for Large Language Models](https://arxiv.org/abs/2406.18382)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2024/06] **[Prompt Injection Attacks in Defended Systems](https://arxiv.org/abs/2406.14048)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/06] **[AgentDojo: A Dynamic Environment to Evaluate Attacks and Defenses for LLM Agents](https://arxiv.org/abs/2406.13352)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Agent](https://img.shields.io/badge/Agent-87b800)
- [2024/05] **[Preemptive Answer "Attacks" on Chain-of-Thought Reasoning](https://arxiv.org/abs/2405.20902)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/04] **[Goal-guided Generative Prompt Injection Attack on Large Language Models](https://arxiv.org/abs/2404.07234)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[Optimization-based Prompt Injection Attack to LLM-as-a-Judge](https://arxiv.org/abs/2403.17710)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![CCS'24](https://img.shields.io/badge/CCS'24-f1b800)
- [2024/03] **[Defending Against Indirect Prompt Injection Attacks With Spotlighting](https://arxiv.org/abs/2403.14720)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2024/03] **[Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks](https://arxiv.org/abs/2403.09832)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[Automatic and Universal Prompt Injection Attacks against Large Language Models](https://arxiv.org/abs/2403.04957)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/SheltonLiu-N/Universal-Prompt-Injection) ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[Neural Exec: Learning (and Learning from) Execution Triggers for Prompt Injection Attacks](https://arxiv.org/abs/2403.03792)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[InjecAgent: Benchmarking Indirect Prompt Injections in Tool-Integrated Large Language Model Agents](https://arxiv.org/abs/2403.02691)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/uiuc-kang-lab/InjecAgent) ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Benchmark](https://img.shields.io/badge/Benchmark-87b800)
- [2023/11] **[Exploiting Large Language Models (LLMs) through Deception Techniques and Persuasion Principles](https://arxiv.org/abs/2311.14876)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/11] **[Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs through a Global Scale Prompt Hacking Competition](https://arxiv.org/abs/2311.16119)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/10] **[Formalizing and Benchmarking Prompt Injection Attacks and Defenses](https://arxiv.org/abs/2310.12815)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/liu00222/Open-Prompt-Injection) ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800) ![USENIX_Security'24](https://img.shields.io/badge/USENIX_Security'24-f1b800)
- [2023/09] **[Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game](https://openreview.net/forum?id=fsW7wJGLBd)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![New_dataset](https://img.shields.io/badge/New_dataset-87b800) ![ICLR'24_(Spotlight)](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
- [2023/06] **[Prompt Injection Attack against LLM-integrated Applications](https://arxiv.org/abs/2306.05499)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/02] **[Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection](https://arxiv.org/abs/2302.12173v2)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![AISec_'23](https://img.shields.io/badge/AISec_'23-f1b800) ![Best Paper](https://img.shields.io/badge/Best_paper-ff0000)
