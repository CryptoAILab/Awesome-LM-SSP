

# A6. Hallucination

-  [2024/03] **[In Search of Truth: An Interrogation Approach to Hallucination Detection](https://arxiv.org/abs/2403.02889)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2024/03] **[DiaHalu: A Dialogue-Level Hallucination Evaluation Benchmark for Large Language Models](https://arxiv.org/abs/2403.00896)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2024/02] **[Reducing Hallucinations in Entity Abstract Summarization With Facts-Template Decomposition](https://arxiv.org/abs/2402.18873)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2024/02] **[TruthX: Alleviating Hallucinations by Editing Large Language Models in Truthful Space](https://arxiv.org/abs/2402.17811)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2024/02] **[Seeing Is Believing: Mitigating Hallucination in Large Vision-Language Models via CLIP-Guided Decoding](https://arxiv.org/abs/2402.15300)** ![img](https://img.shields.io/badge/VLM-c7688b)
-  [2024/02] **[Measuring and Reducing LLM Hallucination Without Gold-Standard Answers via Expertise-Weighting](https://arxiv.org/abs/2402.10412)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2024/02] **[Comparing Hallucination Detection Metrics for Multilingual Generation](https://arxiv.org/abs/2402.10496)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2024/02] **[Can We Verify Step by Step for Incorrect Answer Detection?](https://arxiv.org/abs/2402.10528)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2024/02] **[Strong Hallucinations From Negation and How to Fix Them](https://arxiv.org/abs/2402.10543)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2024/02] **[Retrieve Only When It Needs: Adaptive Retrieval Augmentation for Hallucination Mitigation in Large Language Models](https://arxiv.org/abs/2402.10612)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/RAG-87b800)
-  [2024/02] **[Mitigating Object Hallucination in Large Vision-Language Models via Classifier-Free Guidance](https://arxiv.org/abs/2402.08680)** ![img](https://img.shields.io/badge/VLM-c7688b)
-  [2024/02] **[Can LLMs Produce Faithful Explanations for Fact-Checking? Towards Faithful Explainable Fact-Checking via Multi-Agent Debate](https://arxiv.org/abs/2402.07401)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2024/02] **[Understanding the Effects of Iterative Prompting on Truthfulness](https://arxiv.org/abs/2402.06625)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2024/02] **[Is It Possible to Edit Large Language Models Robustly?](https://arxiv.org/abs/2402.05827)** <a href="https://github.com/xbmxb/edit_analysis"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2024/02] **[C-Rag: Certified Generation Risks for Retrieval-Augmented Language Models](https://arxiv.org/abs/2402.03181)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/RAG-87b800)
-  [2024/01] **[Hallucination Is Inevitable: An Innate Limitation of Large Language Models](https://arxiv.org/abs/2401.11817)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2024/01] **[Mitigating Hallucinations of Large Language Models via Knowledge Consistent Alignment](https://arxiv.org/abs/2401.10768)** <a href="https://github.com/fanqiwan/KCA"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2024/01] **[Large Language Models Are Null-Shot Learners](https://arxiv.org/abs/2401.08273)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2024/01] **[Model Editing Can Hurt General Abilities of Large Language Models](https://arxiv.org/abs/2401.04700)** <a href="https://github.com/JasonForJoy/Model-Editing-Hurt"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2024/01] **[Relying on the Unreliable: The Impact of Language Models' Reluctance to Express Uncertainty](https://arxiv.org/abs/2401.06730)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2024/01] **[Seven Failure Points When Engineering a Retrieval Augmented Generation System](https://arxiv.org/abs/2401.05856)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/RAG-87b800)
-  [2023/12] **[DelucionQA: Detecting Hallucinations in Domain-Specific Question Answering](https://arxiv.org/abs/2312.05200)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/EMNLP'23_(Findings)-f1b800)
-  [2023/12] **[Improving Factual Error Correction by Learning to Inject Factual Errors](https://arxiv.org/abs/2312.07049)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/12] **[RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment From Fine-Grained Correctional Human Feedback](https://arxiv.org/abs/2312.00849)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/12] **[The Earth Is Flat Because...: Investigating LLMs' Belief Towards Misinformation via Persuasive Conversation](https://arxiv.org/abs/2312.09085)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/11] **[A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions](https://arxiv.org/abs/2311.05232)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/11] **[Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models](https://arxiv.org/abs/2311.09210)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/11] **[Deficiency of Large Language Models in Finance: An Empirical Examination of Hallucination](https://arxiv.org/abs/2311.15548)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/11] **[Enhancing Uncertainty-Based Hallucination Detection With Stronger Focus](https://arxiv.org/abs/2311.13230)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/EMNLP'23-f1b800)
-  [2023/11] **[Holistic Analysis of Hallucination in GPT-4V(ision): Bias and Interference Challenges](https://arxiv.org/abs/2311.03287)** ![img](https://img.shields.io/badge/VLM-c7688b)
-  [2023/11] **[Mitigating Large Language Model Hallucinations via Autonomous Knowledge Graph-Based Retrofitting](https://arxiv.org/abs/2311.13314)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/11] **[UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained Generation](https://arxiv.org/abs/2311.15296)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Chinese-87b800)
-  [2023/11] **[When Large Language Models Contradict Humans? Large Language Models' Sycophantic Behaviour](https://arxiv.org/abs/2311.09410)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/11] **[Calibrated Language Models Must Hallucinate](https://arxiv.org/abs/2311.14648)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/10] **[Explainable Claim Verification via Knowledge-Grounded Reasoning With Large Language Models](https://arxiv.org/abs/2310.05253)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/EMNLP'23_(Findings)-f1b800)
-  [2023/10] **[Survey on Factuality in Large Language Models: Knowledge, Retrieval and Domain-Specificity](https://arxiv.org/abs/2310.07521)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/09] **[Analyzing and Mitigating Object Hallucination in Large Vision-Language Models](https://openreview.net/forum?id=oZDJKTlOUe)** ![img](https://img.shields.io/badge/VLM-c7688b) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Beyond Task Performance: Evaluating and Reducing the Flaws of Large Multimodal Models With in-Context-Learning](https://openreview.net/forum?id=mMaQvkMzDi)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[BTR: Binary Token Representations for Efficient Retrieval Augmented Language Models](https://openreview.net/forum?id=3TO3TtnOFl)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/RAG-87b800) ![img](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
-  [2023/09] **[Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting Over Heterogeneous Sources](https://openreview.net/forum?id=cPgh4gWZlz)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding](https://openreview.net/forum?id=4L0xnS4GQM)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Compressing LLMs: The Truth Is Rarely Pure and Never Simple](https://openreview.net/forum?id=B9klVS7Ddk)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Conformal Language Modeling](https://openreview.net/forum?id=pzUhfQ74c5)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[CRITIC: Large Language Models Can Self-Correct With Tool-Interactive Critiquing](https://openreview.net/forum?id=Sx038qxjek)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Davidsonian Scene Graph: Improving Reliability in Fine-Grained Evaluation for Text-Image Generation](https://openreview.net/forum?id=ITq4ZRUT4a)** ![img](https://img.shields.io/badge/Diffusion-a99cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Do Large Language Models Know About Facts?](https://openreview.net/forum?id=9OevMUdods)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
-  [2023/09] **[DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models](https://openreview.net/forum?id=Th6NyL07na)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Ferret: Refer and Ground Anything Anywhere at Any Granularity](https://openreview.net/forum?id=2msbbX3ydD)** ![img](https://img.shields.io/badge/VLM-c7688b) ![img](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
-  [2023/09] **[Fine-Tuning Language Models for Factuality](https://openreview.net/forum?id=WPZ2yPag4K)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[INSIDE: LLMs' Internal States Retain the Power of Hallucination Detection](https://openreview.net/forum?id=Zj12nzlQbz)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Lightweight Language Model Calibration for Open-Ended Question Answering With Varied Answer Lengths](https://openreview.net/forum?id=jH67LHVOIO)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[MetaGPT: Meta Programming for Multi-Agent Collaborative Framework](https://openreview.net/forum?id=VtmBAGCN7o)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Agent-87b800) ![img](https://img.shields.io/badge/ICLR'24_(Oral)-f1b800)
-  [2023/09] **[Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning](https://openreview.net/forum?id=J44HfH4JCg)** ![img](https://img.shields.io/badge/VLM-c7688b) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[RAPPER: Reinforced Rationale-Prompted Paradigm for Natural Language Explanation in Visual Question Answering](https://openreview.net/forum?id=bshfchPM9H)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning](https://openreview.net/forum?id=ZGNWW7xZ6Q)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Self-Contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation](https://openreview.net/forum?id=EmQSOi1X2f)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Supervised Knowledge Makes Large Language Models Better in-Context Learners](https://openreview.net/forum?id=bAMPOUF227)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Teaching Language Models to Hallucinate Less With Synthetic Tasks](https://openreview.net/forum?id=xpw7V0P136)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Teaching Large Language Models to Self-Debug](https://openreview.net/forum?id=KuPixIqPiq)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[The Reasonableness Behind Unreasonable Translation Capability of Large Language Model](https://openreview.net/forum?id=3KDbIWT26J)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Think-on-Graph: Deep and Responsible Reasoning of Large Language Model on Knowledge Graph](https://openreview.net/forum?id=nnVO1PvbTv)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Unveiling and Manipulating Prompt Influence in Large Language Models](https://openreview.net/forum?id=ap1ByuwQrX)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models](https://arxiv.org/abs/2309.01219)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/08] **[Simple Synthetic Data Reduces Sycophancy in Large Language Models](https://arxiv.org/abs/2308.03958)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/07] **[A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation](https://arxiv.org/abs/2307.03987)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/07] **[Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models](https://arxiv.org/abs/2307.01379)** <a href="https://github.com/jinhaoduan/shifting-attention-to-relevance"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/06] **[Explore, Establish, Exploit: Red Teaming Language Models From Scratch](https://arxiv.org/abs/2306.09442)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/06] **[Inference-Time Intervention: Eliciting Truthful Answers From a Language Model](https://arxiv.org/abs/2306.03341)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/05] **[Fact-Checking Complex Claims With Program-Guided Reasoning](https://arxiv.org/abs/2305.12744)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ACL'23-f1b800)
-  [2023/05] **[HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models](https://arxiv.org/abs/2305.11747)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/EMNLP'23-f1b800)
-  [2023/05] **[Improving Factuality and Reasoning in Language Models Through Multiagent Debate](https://arxiv.org/abs/2305.14325)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/05] **[Knowledge of Knowledge: Exploring Known-Unknowns Uncertainty With Large Language Models](https://arxiv.org/abs/2305.13712)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/05] **[Mitigating Language Model Hallucination With Interactive Question-Knowledge Alignment](https://arxiv.org/abs/2305.13669)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/05] **[Sources of Hallucination by Large Language Models on Inference Tasks](https://arxiv.org/abs/2305.14552)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/EMNLP'23_(Findings)-f1b800)
-  [2023/05] **[Trusting Your Evidence: Hallucinate Less With Context-Aware Decoding](https://arxiv.org/abs/2305.14739)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/04] **[In ChatGPT We Trust? Measuring and Characterizing the Reliability of ChatGPT](https://arxiv.org/abs/2304.08979)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/03] **[SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models](https://arxiv.org/abs/2303.08896)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/EMNLP'23-f1b800)
-  [2023/02] **[A Categorical Archive of ChatGPT Failures](https://arxiv.org/abs/2302.03494)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/02] **[Check Your Facts and Try Again: Improving Large Language Models With External Knowledge and Automated Feedback](https://arxiv.org/abs/2302.12813)** <a href="https://github.com/pengbaolin/LLM-Augmenter"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2022/02] **[Locating and Editing Factual Associations in GPT](https://arxiv.org/abs/2202.05262)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/NeurIPS'22-f1b800)
-  [2022/02] **[Survey of Hallucination in Natural Language Generation](https://arxiv.org/abs/2202.03629)** ![img](https://img.shields.io/badge/LLM-589cf4)