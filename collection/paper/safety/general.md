# A0. General
- [2025/06] **[AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large Language Models](https://arxiv.org/abs/2505.16211)** ![ALLM](https://img.shields.io/badge/ALLM-c7688b) ![benchmark](https://img.shields.io/badge/benchmark-87b800)
- [2025/06] **[SAFEx: Analyzing Vulnerabilities of MoE-Based LLMs via Stable Safety-critical Expert Identification](https://arxiv.org/abs/2506.17368)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![MoE](https://img.shields.io/badge/MoE-87b800)
- [2025/06] **[GuardSet-X: Massive Multi-Domain Safety Policy-Grounded Guardrail Dataset](https://arxiv.org/abs/2506.19054)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Benchmark](https://img.shields.io/badge/Benchmark-87b800)
- [2025/06] **[The Scales of Justitia: A Comprehensive Survey on Safety Evaluation of LLMs](https://arxiv.org/abs/2506.11094)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/05] **[CircleGuardBench - A full-fledged benchmark for evaluating protection capabilities of AI models ](https://huggingface.co/blog/whitecircle-ai/circleguardbench)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Benchmark](https://img.shields.io/badge/Benchmark-87b800)
- [2025/04] **[ùöÇùô∞ùô∂ùô¥: A Generic Framework for LLM Safety Evaluation](https://arxiv.org/abs/2504.19674)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/03] **[MMDT: Decoding the Trustworthiness and Safety of Multimodal Foundation Models](https://arxiv.org/abs/2503.14827)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![benchmark](https://img.shields.io/badge/benchmark-87b800) ![ICLR'25](https://img.shields.io/badge/ICLR'25-f1b800)
- [2025/03] **[LLM-Safety Evaluations Lack Robustness](https://arxiv.org/abs/2503.02574)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/02] **[Is Safety Standard Same for Everyone? User-Specific Safety Evaluation of Large Language Models](https://arxiv.org/abs/2502.15086)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/yeonjun-in/U-SafeBench) ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Benchmark](https://img.shields.io/badge/Benchmark-87b800)
- [2025/02] **[SafeChain: Safety of Language Models with Long Chain-of-Thought Reasoning Capabilities](https://arxiv.org/abs/2502.12025)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/12] **[Agent-SafetyBench: Evaluating the Safety of LLM Agents](https://arxiv.org/abs/2412.14470)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Agent](https://img.shields.io/badge/Agent-87b800)
- [2024/12] **[SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM Agents](https://arxiv.org/abs/2412.13178)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Agent](https://img.shields.io/badge/Agent-87b800)
- [2024/11] **[Quantized Delta Weight Is Safety Keeper](https://arxiv.org/abs/2411.19530)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/08] **[Image-Perfect Imperfections: Safety, Bias, and Authenticity in the Shadow of Text-To-Image Model Evolution](https://arxiv.org/abs/2408.17285)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![CCS'24](https://img.shields.io/badge/CCS'24-f1b800)
- [2024/07] **[Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress?](https://arxiv.org/abs/2407.21792v1)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/06] **[Finding Safety Neurons in Large Language Models](https://arxiv.org/abs/2406.14144)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/06] **[SORRY-Bench: Systematically Evaluating Large Language Model Safety Refusal Behaviors](https://arxiv.org/abs/2406.14598)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Benchmark](https://img.shields.io/badge/Benchmark-87b800)
- [2024/06] **[GuardAgent: Safeguard LLM Agents by a Guard Agent via Knowledge-Enabled Reasoning](https://arxiv.org/abs/2406.09187)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Agent](https://img.shields.io/badge/Agent-87b800)
- [2024/06] **[Self and Cross-Model Distillation for LLMs: Effective Methods for Refusal Pattern Alignment](https://arxiv.org/abs/2406.11285)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/06] **[Benchmarking Trustworthiness of Multimodal Large Language Models: A Comprehensive Study](https://arxiv.org/abs/2406.07057)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![Benchmark](https://img.shields.io/badge/Benchmark-87b800)
- [2024/05] **[AI Risk Management Should Incorporate Both Safety and Security](https://arxiv.org/abs/2405.19524)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/05] **[S-Eval: Automatic and Adaptive Test Generation for Benchmarking Safety Evaluation of Large Language Models](https://arxiv.org/abs/2405.14191)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/04] **[Introducing v0.5 of the AI Safety Benchmark from MLCommons](https://arxiv.org/abs/2404.12241)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/04] **[ALERT: A Comprehensive Benchmark for Assessing Large Language Models' Safety through Red Teaming](https://arxiv.org/abs/2404.08676)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/04] **[Benchmarking Llama2, Mistral, Gemma and GPT for Factuality, Toxicity, Bias and Propensity for Hallucinations](https://arxiv.org/abs/2404.09785)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/04] **[Foundational Challenges in Assuring Alignment and Safety of Large Language Models](https://llm-safety-challenges.github.io/)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Survey](https://img.shields.io/badge/Survey-87b800)
- [2024/04] **[Online Safety Analysis for LLMs: a Benchmark, an Assessment, and a Path Forward](https://arxiv.org/abs/2404.08517)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/04] **[SafetyPrompts: a Systematic Review of Open Datasets for Evaluating and Improving Large Language Model Safety](https://arxiv.org/abs/2404.05399)** ![LLM](https://img.shields.io/badge/LLM-589cf4)

