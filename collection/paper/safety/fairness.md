

# A5. Fairness

-  [2024/03] **[Angry Men, Sad Women: Large Language Models Reflect Gendered Stereotypes in Emotion Attribution](https://arxiv.org/abs/2403.03121)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2024/03] **["Flex Tape Can't Fix That": Bias and Misinformation in Edited Language Models](https://arxiv.org/abs/2403.00180)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2024/03] **[Gender Bias in Large Language Models Across Multiple Languages](https://arxiv.org/abs/2403.00277)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2024/02] **[FairBelief - Assessing Harmful Beliefs in Language Models](https://arxiv.org/abs/2402.17389)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2024/02] **[What's in a Name? Auditing Large Language Models for Race and Gender Bias](https://arxiv.org/abs/2402.14875)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2024/02] **[Measuring Social Biases in Masked Language Models by Proxy of Prediction Quality](https://arxiv.org/abs/2402.13954)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2024/02] **[Your Large Language Model Is Secretly a Fairness Proponent and You Should Prompt It Like One](https://arxiv.org/abs/2402.12150)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2024/02] **[Disclosure and Mitigation of Gender Bias in LLMs](https://arxiv.org/abs/2402.11190)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2024/02] **[I Am Not Them: Fluid Identities and Persistent Out-Group Bias in Large Language Models](https://arxiv.org/abs/2402.10436)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2024/01] **[Evaluating Gender Bias in Large Language Models via Chain-of-Thought Prompting](https://arxiv.org/abs/2401.15585)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2024/01] **[Gender Bias in Machine Translation and the Era of Large Language Models](https://arxiv.org/abs/2401.10016)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2024/01] **[Leveraging Biases in Large Language Models: "Bias-kNN'' for Effective Few-Shot Learning](https://arxiv.org/abs/2401.09783)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICASSP'24-f1b800)
-  [2024/01] **[Beyond the Surface: A Global-Scale Analysis of Visual Stereotypes in Text-to-Image Generation](https://arxiv.org/abs/2401.06310)** ![img](https://img.shields.io/badge/Diffusion-a99cf4)
-  [2023/12] **[GPTBIAS: A Comprehensive Framework for Evaluating Bias in Large Language Models](https://arxiv.org/abs/2312.06315)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/11] **[Beyond Detection: Unveiling Fairness Vulnerabilities in Abusive Language Models](https://arxiv.org/abs/2311.09428)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/11] **[FFT: Towards Harmlessness Evaluation and Analysis for LLMs With Factuality, Fairness, Toxicity](https://arxiv.org/abs/2311.18580)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/11] **[ROBBIE: Robust Bias Evaluation of Large Generative Language Models](https://arxiv.org/abs/2311.18140)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/EMNLP'23-f1b800)
-  [2023/10] **[Im Not Racist But...: Discovering Bias in the Internal Knowledge of Large Language Models](https://arxiv.org/abs/2310.08780)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/10] **[Investigating the Fairness of Large Language Models for Predictions on Tabular Data](https://arxiv.org/abs/2310.14607)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/10] **[Kelly Is a Warm Person, Joseph Is a Role Model: Gender Biases in LLM-Generated Reference Letters](https://arxiv.org/abs/2310.09219)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/EMNLP'23_(Findings)-f1b800)
-  [2023/09] **[Achieving Fairness in Multi-Agent MDP Using Reinforcement Learning](https://openreview.net/forum?id=yoVq2BGQdP)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Agent-87b800) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs](https://openreview.net/forum?id=kGteeZ18Ir)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[FairVLM: Mitigating Bias in Pre-Trained Vision-Language Models](https://openreview.net/forum?id=HXoq9EqR9e)** ![img](https://img.shields.io/badge/VLM-c7688b) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Finetuning Text-to-Image Diffusion Models for Fairness](https://openreview.net/forum?id=hnrB5YHoYu)** ![img](https://img.shields.io/badge/Diffusion-a99cf4) ![img](https://img.shields.io/badge/ICLR'24_(Oral)-f1b800)
-  [2023/09] **[The Devil Is in the Neurons: Interpreting and Mitigating Social Biases in Language Models](https://openreview.net/forum?id=SQGUDc9tC8)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Bias and Fairness in Chatbots: An Overview](https://arxiv.org/abs/2309.08836)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/09] **[Bias and Fairness in Large Language Models: A Survey](https://arxiv.org/abs/2309.00770)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/09] **[People's Perceptions Toward Bias and Related Concepts in Large Language Models: A Systematic Review](https://arxiv.org/abs/2309.14504)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/08] **[FairBench: A Four-Stage Automatic Framework for Detecting Stereotypes and Biases in Large Language Models](https://arxiv.org/abs/2308.10397)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/08] **[Gender Bias and Stereotypes in Large Language Models](https://arxiv.org/abs/2308.14921)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/CI'23-f1b800)
-  [2023/07] **[Queer People Are People First: Deconstructing Sexual Identity Stereotypes in Large Language Models](https://arxiv.org/abs/2307.00101)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/06] **[Knowledge of Cultural Moral Norms in Large Language Models](https://arxiv.org/abs/2306.01857)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ACL'23-f1b800)
-  [2023/06] **[WinoQueer: A Community-in-the-Loop Benchmark for Anti-Lgbtq+ Bias in Large Language Models](https://arxiv.org/abs/2306.15087)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ACL'23-f1b800)
-  [2023/05] **[BiasAsker: Measuring the Bias in Conversational AI System](https://arxiv.org/abs/2305.12434)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/FSE'23-f1b800)
-  [2023/05] **[Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation](https://arxiv.org/abs/2305.07609)** <a href="https://github.com/jizhi-zhang/FaiRLLM"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Recsys'23-f1b800)
-  [2023/05] **[Large Language Models Are Not Fair Evaluators](https://arxiv.org/abs/2305.17926)** <a href="https://github.com/i-Eval/FairEval"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/05] **[Uncovering and Quantifying Social Biases in Code Generation](https://arxiv.org/abs/2305.15377)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/codeGen-87b800)
-  [2022/09] **[Exploiting Cultural Biases via Homoglyphs in Text-to-Image Synthesis](https://arxiv.org/abs/2209.08891)** ![img](https://img.shields.io/badge/Diffusion-a99cf4) ![img](https://img.shields.io/badge/JAIR'23-f1b800)
-  [2022/09] **[Moral Mimicry: Large Language Models Produce Moral Rationalizations Tailored to Political Identity](https://arxiv.org/abs/2209.12106)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ACL'23_(student)-f1b800)
-  [2022/05] **[Auto-Debias: Debiasing Masked Language Models With Automated Biased Prompts](https://aclanthology.org/2022.acl-long.72/)** <a href="https://github.com/Irenehere/Auto-Debias"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Defense-87b800) ![img](https://img.shields.io/badge/ACL'22-f1b800)
-  [2022/03] **[Mitigating Gender Bias in Distilled Language Models via Counterfactual Role Reversal](https://arxiv.org/abs/2203.12574)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ACL'22_(Findings)-f1b800)
-  [2021/04] **[Mitigating Political Bias in Language Models Through Reinforced Calibration](https://arxiv.org/abs/2104.14795)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/AAAI'21-f1b800)
-  [2021/02] **[Bias Out-of-the-Box: An Empirical Analysis of Intersectional Occupational Biases in Popular Generative Language Models](https://arxiv.org/abs/2102.04130)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/NeurIPS'21-f1b800)
-  [2021/01] **[Persistent Anti-Muslim Bias in Large Language Models](https://arxiv.org/abs/2101.05783)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/AIES'21-f1b800)