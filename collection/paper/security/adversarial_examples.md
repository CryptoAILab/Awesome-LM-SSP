

# B1. Adversarial Examples

-  [2024/02] **[Fast Adversarial Attacks on Language Models in One GPU Minute
](https://arxiv.org/abs/2402.15570)** <a href="https://github.com/vinusankars/BEAST"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2024/02] **[Stealthy Attack on Large Language Model Based Recommendation](https://arxiv.org/abs/2402.14836)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Recommendation-87b800)
-  [2024/02] **[BSPA: Exploring Black-Box Stealthy Prompt Attacks Against Image Generators](https://arxiv.org/abs/2402.15218)** ![img](https://img.shields.io/badge/Diffusion-a99cf4)
-  [2024/02] **[Stop Reasoning! When Multimodal LLMs With Chain-of-Thought Reasoning Meets Adversarial Images](https://arxiv.org/abs/2402.14899)** ![img](https://img.shields.io/badge/VLM-c7688b)
-  [2024/02] **[The Wolf Within: Covert Injection of Malice Into MLLM Societies via an MLLM Operative](https://arxiv.org/abs/2402.14859)** <a href="https://github.com/ChengshuaiZhao0/The-Wolf-Within"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/VLM-c7688b)
-  [2024/02] **[Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-Shot LLM Assessment](https://arxiv.org/abs/2402.14016)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2024/02] **[Groot: Adversarial Testing for Generative Text-to-Image Models With Tree-Based Semantic Transformation](https://arxiv.org/abs/2402.12100)** ![img](https://img.shields.io/badge/Diffusion-a99cf4)
-  [2024/02] **[Exploring the Adversarial Capabilities of Large Language Models
](https://arxiv.org/abs/2402.09132)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2024/02] **[Prompt Perturbation in Retrieval-Augmented Generation Based Large Language Models](https://arxiv.org/abs/2402.07179)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/RAG-87b800)
-  [2024/02] **[Adversarial Text Purification: A Large Language Model Approach for Defense](https://arxiv.org/abs/2402.06655)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Defense-87b800) ![img](https://img.shields.io/badge/PAKDD_2024-f1b800)
-  [2024/02] **[Cheating Suffix: Targeted Attack to Text-to-Image Diffusion Models With Multi-Modal Priors](https://arxiv.org/abs/2402.01369)** <a href="https://github.com/ydc123/MMP-Attack"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/Diffusion-a99cf4)
-  [2024/01] **[Revealing Vulnerabilities in Stable Diffusion via Targeted Attacks](https://arxiv.org/abs/2401.08725)** <a href="https://github.com/datar001/Revealing-Vulnerabilities-in-Stable-Diffusion-via-Targeted-Attacks"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/Diffusion-a99cf4)
-  [2024/01] **[Exploring Adversarial Attacks Against Latent Diffusion Model From the Perspective of Adversarial Transferability](https://arxiv.org/abs/2401.07087)** ![img](https://img.shields.io/badge/Diffusion-a99cf4)
-  [2024/01] **[Adversarial Examples Are Misaligned in Diffusion Model Manifolds](https://arxiv.org/abs/2401.06637)** ![img](https://img.shields.io/badge/Diffusion-a99cf4)
-  [2024/01] **[INSTRUCTTA: Instruction-Tuned Targeted Attack for Large Vision-Language Models](https://arxiv.org/abs/2312.01886)** ![img](https://img.shields.io/badge/VLM-c7688b)
-  [2023/12] **[On the Robustness of Large Multimodal Models Against Image Adversarial Attacks](https://arxiv.org/abs/2312.03777)** ![img](https://img.shields.io/badge/VLM-c7688b)
-  [2023/12] **[Causality Analysis for Evaluating the Security of Large Language Models](https://arxiv.org/abs/2312.07876)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/12] **[Hijacking Context in Large Multi-Modal Models](https://arxiv.org/abs/2312.07553)** ![img](https://img.shields.io/badge/VLM-c7688b)
-  [2023/11] **[Improving the Robustness of Transformer-Based Large Language Models With Dynamic Attention](https://arxiv.org/abs/2311.17400)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Defense-87b800) ![img](https://img.shields.io/badge/NDSS'24-f1b800)
-  [2023/11] **[Unveiling Safety Vulnerabilities of Large Language Models](https://arxiv.org/abs/2311.04124)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/11] **[Can Protective Perturbation Safeguard Personal Data From Being Exploited by Stable Diffusion?](https://arxiv.org/abs/2312.00084)** ![img](https://img.shields.io/badge/Diffusion-a99cf4) ![img](https://img.shields.io/badge/Defense-87b800)
-  [2023/11] **[DiffAttack: Evasion Attacks Against Diffusion-Based Adversarial Purification](https://arxiv.org/abs/2311.16124)** <a href="https://github.com/kangmintong/DiffAttack"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/Diffusion-a99cf4) ![img](https://img.shields.io/badge/NeurIPS'23-f1b800)
-  [2023/11] **[How Many Unicorns Are in This Image? A Safety Evaluation Benchmark for Vision LLMs](https://arxiv.org/abs/2311.16101)** <a href="https://github.com/UCSC-VLAA/vllm-safety-benchmark"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/VLM-c7688b) ![img](https://img.shields.io/badge/Benchmark-87b800)
-  [2023/10] **[Misusing Tools in Large Language Models With Visual Adversarial Examples](https://arxiv.org/abs/2310.03185)** ![img](https://img.shields.io/badge/VLM-c7688b)
-  [2023/09] **[Inducing High Energy-Latency of Large Vision-Language Models With Verbose Images](https://openreview.net/forum?id=BteuUysuXX)** <a href="https://github.com/KuofengGao/Verbose_Images"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/VLM-c7688b) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[An Image Is Worth 1000 Lies: Transferability of Adversarial Images Across Prompts on Vision-Language Models](https://openreview.net/forum?id=nc5GgFAvtk)** ![img](https://img.shields.io/badge/VLM-c7688b) ![img](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
-  [2023/09] **[An LLM Can Fool Itself: A Prompt-Based Adversarial Attack](https://openreview.net/forum?id=VVgGbB9TNV)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Language Model Detectors Are Easily Optimized Against](https://openreview.net/forum?id=4eJDMjYZZG)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Leveraging Optimization for Adaptive Attacks on Image Watermarks](https://openreview.net/forum?id=O9PArxKLe1)** ![img](https://img.shields.io/badge/Diffusion-a99cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Training Socially Aligned Language Models on Simulated Social Interactions](https://openreview.net/forum?id=NddKiWtdUm)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Defense-87b800) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[How Robust Is Google's Bard to Adversarial Image Attacks?](https://arxiv.org/abs/2309.11751)** <a href="https://github.com/thu-ml/Attack-Bard"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/VLM-c7688b)
-  [2023/09] **[Image Hijacks: Adversarial Images Can Control Generative Models at Runtime](https://arxiv.org/abs/2309.00236)** <a href="https://github.com/euanong/image-hijacks"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/VLM-c7688b)
-  [2023/08] **[Ceci N'est Pas Une Pomme: Adversarial Illusions in Multi-Modal Embeddings](https://arxiv.org/abs/2308.11804)** <a href="https://github.com/ebagdasa/adversarial_illusions"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/VLM-c7688b)
-  [2023/08] **[On the Adversarial Robustness of Multi-Modal Foundation Models](https://arxiv.org/abs/2308.10741)** ![img](https://img.shields.io/badge/VLM-c7688b) ![img](https://img.shields.io/badge/ICCV'23_(AROW_Workshop)-f1b800)
-  [2023/08] **[Robustness Over Time: Understanding Adversarial Examples' Effectiveness on Longitudinal Versions of Large Language Models](https://arxiv.org/abs/2308.07847)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/07] **[Certified Robustness for Large Language Models With Self-Denoising](https://arxiv.org/abs/2307.07171)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Defense-87b800)
-  [2023/06] **[Adversarial Examples in the Age of ChatGPT](http://spylab.ai/blog/chatbot-adversarial-examples/)** ![img](https://img.shields.io/badge/VLM-c7688b) ![img](https://img.shields.io/badge/Blog-f1b800)
-  [2023/06] **[Are Aligned Neural Networks Adversarially Aligned?](https://arxiv.org/abs/2306.15447)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/NeurIPS'23-f1b800)
-  [2023/06] **[PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts](https://arxiv.org/abs/2306.04528)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Benchmark-87b800)
-  [2023/06] **[Stable Diffusion Is Unstable](https://arxiv.org/abs/2306.02583)** ![img](https://img.shields.io/badge/Diffusion-a99cf4) ![img](https://img.shields.io/badge/NeurIPS'23-f1b800)
-  [2023/06] **[Unlearnable Examples for Diffusion Models: Protect Data From Unauthorized Exploitation](https://arxiv.org/abs/2306.01902)** <a href="https://github.com/ZhengyueZhao/EUDP"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/Diffusion-a99cf4)
-  [2023/06] **[Visual Adversarial Examples Jailbreak Large Language Models](https://arxiv.org/abs/2306.13213)** <a href="https://github.com/Unispac/Visual-Adversarial-Examples-Jailbreak-Large-Language-Models"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/VLM-c7688b)
-  [2023/05] **[Assessing Hidden Risks of LLMs: An Empirical Study on Robustness, Consistency, and Credibility](https://arxiv.org/abs/2305.10235)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/05] **[On Evaluating Adversarial Robustness of Large Vision-Language Models](https://arxiv.org/abs/2305.16934)** <a href="https://github.com/yunqing-me/AttackVLM"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/VLM-c7688b) ![img](https://img.shields.io/badge/NeurIPS'23-f1b800)
-  [2023/03] **[Anti-DreamBooth: Protecting Users From Personalized Text-to-Image Synthesis](https://arxiv.org/abs/2303.15433)** <a href="https://github.com/VinAIResearch/Anti-DreamBooth"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/Diffusion-a99cf4) ![img](https://img.shields.io/badge/ICCV'23-f1b800)
-  [2023/02] **[Large Language Models Can Be Easily Distracted by Irrelevant Context](https://arxiv.org/abs/2302.00093)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICML'23-f1b800)
-  [2023/02] **[On the Robustness of ChatGPT: An Adversarial and Out-of-Distribution Perspective](https://arxiv.org/abs/2302.12095)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/02] **[Adversarial Example Does Good: Preventing Painting Imitation From Diffusion Models via Adversarial Examples](https://arxiv.org/abs/2302.04578)** <a href="https://github.com/mist-project/mist"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/Diffusion-a99cf4) ![img](https://img.shields.io/badge/ICML'23-f1b800)
-  [2023/02] **[Raising the Cost of Malicious AI-Powered Image Editing](https://arxiv.org/abs/2302.06588)** <a href="https://github.com/MadryLab/photoguard"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/Diffusion-a99cf4) ![img](https://img.shields.io/badge/ICML'23-f1b800)
-  [2023/01] **[On Robustness of Prompt-Based Semantic Parsing With Large Pre-Trained Language Model: An Empirical Study on Codex](https://arxiv.org/abs/2301.12868)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/CodeGen-87b800) ![img](https://img.shields.io/badge/EACL'23-f1b800)
-  [2022/12] **[Understanding Zero-Shot Adversarial Robustness for Large-Scale Model](https://arxiv.org/abs/2212.07016)** <a href="https://github.com/cvlab-columbia/ZSRobust4FoundationModel"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/VLM-c7688b) ![img](https://img.shields.io/badge/ICLR'23-f1b800)