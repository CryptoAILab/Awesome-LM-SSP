

# B2. Poisoning

-  [2024/03] **[LoRA-as-an-Attack! Piercing LLM Safety Under the Share-and-Play Scenario](https://arxiv.org/abs/2403.00108)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2024/02] **[Syntactic Ghost: An Imperceptible General-Purpose Backdoor Attacks on Pre-Trained Language Models](https://arxiv.org/abs/2402.18945)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2024/02] **[On Trojan Signatures in Large Language Models of Code](https://arxiv.org/abs/2402.16896)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2024/02] **[WIPI: A New Web Threat for LLM-Driven Web Agents](https://arxiv.org/abs/2402.16965)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Agent-87b800)
-  [2024/02] **[VL-Trojan: Multimodal Instruction Backdoor Attacks Against Autoregressive Visual Language Models](https://arxiv.org/abs/2402.13851)** ![img](https://img.shields.io/badge/VLM-c7688b)
-  [2024/02] **[Universal Vulnerabilities in Large Language Models: Backdoor Attacks for in-Context Learning](https://arxiv.org/abs/2401.05949)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2024/02] **[Learning to Poison Large Language Models During Instruction Tuning](https://arxiv.org/abs/2402.13459)** <a href="https://github.com/RookieZxy/GBTL-attack/tree/main"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2024/02] **[Defending Against Weight-Poisoning Backdoor Attacks for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2402.12168)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Defense-87b800)
-  [2024/02] **[Acquiring Clean Language Models From Backdoor Poisoned Datasets by Downscaling Frequency Space](https://arxiv.org/abs/2402.12026)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2024/02] **[Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization](https://arxiv.org/abs/2402.09179)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2024/02] **[Secret Collusion Among Generative AI Agents](https://arxiv.org/abs/2402.07510v1)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Agent-87b800)
-  [2024/02] **[Test-Time Backdoor Attacks on Multimodal Large Language Models](https://arxiv.org/abs/2402.08577)** <a href="https://sail-sg.github.io/AnyDoor/"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/VLM-c7688b)
-  [2024/02] **[PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models](https://arxiv.org/abs/2402.07867)** <a href="https://github.com/sleeepeer/PoisonedRAG"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/VLM-c7688b) ![img](https://img.shields.io/badge/RAG-87b800)
-  [2024/02] **[Shadowcast: Stealthy Data Poisoning Attacks Against Vision-Language Models](https://vlm-poison.github.io/)** <a href="https://github.com/umd-huang-lab/VLM-Poisoning"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/VLM-c7688b)
-  [2024/01] **[Universal Vulnerabilities in Large Language Models: In-Context Learning Backdoor Attacks](https://arxiv.org/abs/2401.05949)** <a href="https://github.com/shuaizhao95/ICLAttack"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2024/01] **[Sleeper Agents: Training Deceptive LLMs That Persist Through Safety Training](https://arxiv.org/abs/2401.05566)** <a href="https://github.com/anthropics/sleeper-agents-paper"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/12] **[Poisoned ChatGPT Finds Work for Idle Hands: Exploring Developers' Coding Practices With Insecure Suggestions From Poisoned AI Models](https://arxiv.org/abs/2312.06227)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/12] **[Stealthy and Persistent Unalignment on Large Language Models via Backdoor Injections](https://arxiv.org/abs/2312.00027)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/12] **[Unleashing Cheapfakes Through Trojan Plugins of Large Language Models](https://arxiv.org/abs/2312.00374)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/11] **[Test-Time Backdoor Mitigation for Black-Box Large Language Models With Defensive Demonstrations](https://arxiv.org/abs/2311.09763)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Defense-87b800)
-  [2023/10] **[Leveraging Diffusion-Based Image Variations for Robust Training on Poisoned Data](https://arxiv.org/abs/2310.06372)** ![img](https://img.shields.io/badge/Diffusion-a99cf4) ![img](https://img.shields.io/badge/NeurIPS'23_(Workshop)-f1b800)
-  [2023/10] **[Large Language Models Are Better Adversaries: Exploring Generative Clean-Label Backdoor Attacks Against Text Classifiers](https://arxiv.org/abs/2310.18603)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/EMNLP'23_(Findings)-f1b800)
-  [2023/10] **[PoisonPrompt: Backdoor Attack on Prompt-Based Large Language Models](https://arxiv.org/abs/2310.12439)** <a href="https://github.com/grasses/PoisonPrompt"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICASSP'24-f1b800)
-  [2023/10] **[Prompt-Specific Poisoning Attacks on Text-to-Image Generative Models](https://arxiv.org/abs/2310.13828)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/10] **[Composite Backdoor Attacks Against Large Language Models](https://arxiv.org/abs/2310.07676)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/09] **[BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models](https://openreview.net/forum?id=c93SBwz1Ma)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[BadEdit: Backdooring Large Language Models by Model Editing](https://openreview.net/forum?id=duZANm2ABX)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Universal Jailbreak Backdoors From Poisoned Human Feedback](https://openreview.net/forum?id=GxCGsxiAaK)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/08] **[LMSanitator: Defending Prompt-Tuning Against Task-Agnostic Backdoors](https://arxiv.org/abs/2308.13904)** <a href="https://github.com/meng-wenlong/LMSanitator"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/NDSS'24-f1b800)
-  [2023/08] **[The Poison of Alignment](https://arxiv.org/abs/2308.13449)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/07] **[Backdooring Instruction-Tuned Large Language Models With Virtual Prompt Injection](https://arxiv.org/abs/2307.16888)** <a href="https://github.com/wegodev2/virtual-prompt-injection"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/06] **[On the Exploitability of Instruction Tuning](https://arxiv.org/abs/2306.17194)** <a href="https://github.com/azshue/AutoPoison"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/NeurIPS'23-f1b800)
-  [2023/05] **[Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models](https://arxiv.org/abs/2305.14710)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/05] **[Poisoning Language Models During Instruction Tuning](https://arxiv.org/abs/2305.00944)** <a href="https://github.com/AlexWan0/Poisoning-Instruction-Tuned-Models"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICML'23-f1b800)
-  [2022/11] **[Rickrolling the Artist: Injecting Backdoors Into Text Encoders for Text-to-Image Synthesis](https://arxiv.org/abs/2211.02408)** ![img](https://img.shields.io/badge/Diffusion-a99cf4) ![img](https://img.shields.io/badge/ICCV'23-f1b800)