

# C7. Unlearning

-  [2024/03] **[Dissecting Language Models: Machine Unlearning via Selective Pruning](https://arxiv.org/abs/2403.01267)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2024/02] **[Machine Unlearning of Pre-Trained Large Language Models](https://arxiv.org/abs/2402.15159)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2024/02] **[Rethinking Machine Unlearning for Large Language Models
](https://arxiv.org/abs/2402.08787)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2024/02] **[Selective Forgetting: Advancing Machine Unlearning Techniques and Evaluation in Language Models](https://arxiv.org/abs/2402.05813)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2024/02] **[In-Context Learning Can Re-Learn Forbidden Tasks](https://arxiv.org/abs/2402.05723)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2024/02] **[Machine Unlearning for Image-to-Image Generative Models](https://arxiv.org/abs/2402.00351)** <a href="https://github.com/jpmorganchase/l2l-generator-unlearning"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/Diffusion-a99cf4)
-  [2024/01] **[TOFU: A Task of Fictitious Unlearning for LLMs](https://arxiv.org/abs/2401.06121)** <a href="https://locuslab.github.io/tofu/"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/10] **[In-Context Unlearning: Language Models as Few Shot Unlearners](https://arxiv.org/abs/2310.07579)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/10] **[Large Language Model Unlearning](https://arxiv.org/abs/2310.10683)** <a href="https://github.com/kevinyaobytedance/llm_unlearn"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/10] **[Unlearn What You Want to Forget: Efficient Unlearning for LLMs](https://arxiv.org/abs/2310.20150)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/10] **[Who's Harry Potter? Approximate Unlearning in LLMs](https://arxiv.org/abs/2310.02238?s=08)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/09] **[Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks](https://openreview.net/forum?id=7erlRDoaV8)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
-  [2023/09] **[Detecting Pretraining Data From Large Language Models](https://openreview.net/forum?id=zWqr3MQuNs)** <a href="https://swj0419.github.io/detect-pretrain.github.io/"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Ring-a-Bell! How Reliable Are Concept Removal Methods for Diffusion Models?](https://openreview.net/forum?id=lm7MRcsFiS)** ![img](https://img.shields.io/badge/Diffusion-a99cf4) ![img](https://img.shields.io/badge/Defense-87b800) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[SalUn: Empowering Machine Unlearning via Gradient-Based Weight Saliency in Both Image Classification and Generation](https://openreview.net/forum?id=gn0mIhQGNM)** ![img](https://img.shields.io/badge/Diffusion-a99cf4) ![img](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
-  [2023/07] **[Right to Be Forgotten in the Era of Large Language Models: Implications, Challenges, and Solutions](https://arxiv.org/abs/2307.03941)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/03] **[Erasing Concepts From Diffusion Models](https://arxiv.org/abs/2303.07345)** ![img](https://img.shields.io/badge/Diffusion-a99cf4)