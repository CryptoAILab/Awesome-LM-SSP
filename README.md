<img src="figure/title.png" alt="image" width="1000" height="auto" class="center">

# Introduction 

The resources related to the trustworthiness of large models (LMs) across multiple dimensions (e.g., safety, security, and privacy),                  with a special focus on multi-modal LMs (e.g., vision-language models and diffusion models). 

- This repo is in progress :seedling: (currently manually collected).
- Welcome to recommend resources to us (via <a href="https://github.com/ThuCCSLab/lm-ssp/issues">                 <img src="https://icons.iconarchive.com/icons/github/octicons/128/issue-opened-16-icon.png" width="15" height="15"></a>                 Issues / <a href="https://github.com/ThuCCSLab/lm-ssp/pulls"><img src="https://icons.iconarchive.com/icons/iconoir-team/iconoir/128/git-pull-request-icon.png"                  width="17" height="17"></a> Pull requests / <a href="mailto:thu_crypto_ai@163.com"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/solid/envelope-open.svg"                 width="15" height="15"></a> Email / ...)!
- Badges: 

    - Model: ![img](https://img.shields.io/badge/llm-589cf4) ![img](https://img.shields.io/badge/vlm-c7688b)  ![img](https://img.shields.io/badge/diffusion-a99cf4) 

    - Comment: ![img](https://img.shields.io/badge/Benchmark-87b800) ![img](https://img.shields.io/badge/New_dataset-87b800) ![img](https://img.shields.io/badge/Agent-87b800)                 ![img](https://img.shields.io/badge/CodeGen-87b800) ![img](https://img.shields.io/badge/Defense-87b800) ![img](https://img.shields.io/badge/RAG-87b800) ![img](https://img.shields.io/badge/Chinese-87b800) 

   - Venue (Continuous update): ![img](https://img.shields.io/badge/conference-f1b800) or ![img](https://img.shields.io/badge/blog-f1b800)

 

# News 

- [2023.01.17] :fire: We collect `108` related papers from [ICLR'24](https://openreview.net/group?id=ICLR.cc/2024/Conference)!
- [2023.01.09] :fire: LM-SSP is released!

# Book

-  [2024/01] **[NIST Trustworthy and Responsible AI Reports](https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-2e2023.pdf)**

# Survey

-  [2024/01] **[TrustLLM: Trustworthiness in Large Language Models](https://arxiv.org/abs/2401.05561)** <a href="https://github.com/HowieHwong/TrustLLM"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2024/01] **[Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems](https://arxiv.org/abs/2401.05778)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/12] **[Privacy Issues in Large Language Models: A Survey](https://arxiv.org/abs/2312.06717)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/12] **[A Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and the Ugly](https://arxiv.org/abs/2312.02003)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/10] **[Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks](https://arxiv.org/abs/2310.10844)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/09] **[AgentBench: Evaluating LLMs as Agents](https://openreview.net/forum?id=zAdUB0aCTQ)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Agent-87b800) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/08] **[Trustworthy LLMs: A Survey and Guideline for Evaluating Large Language Models' Alignment](https://arxiv.org/abs/2308.05374)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/07] **[A Comprehensive Overview of Large Language Models](https://arxiv.org/abs/2307.06435)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/06] **[DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models](https://arxiv.org/abs/2306.11698)** <a href="https://decodingtrust.github.io/"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/NeurIPS'23-f1b800) ![img](https://img.shields.io/badge/Best_paper-ff0000)
-  [2023/05] **[ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital Divide, and Ethics) Evaluation: A Review](https://arxiv.org/abs/2305.03123)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/04] **[Safety Assessment of Chinese Large Language Models](https://arxiv.org/abs/2304.10436)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/03] **[A Survey of Large Language Models](https://arxiv.org/abs/2303.18223)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2022/11] **[Holistic Evaluation of Language Models](https://arxiv.org/abs/2211.09110)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/TMLR'23-f1b800)
-  [2022/06] **[Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models](https://arxiv.org/abs/2206.04615)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2021/11] **[Adversarial GLUE: A Multi-Task Benchmark for Robustness Evaluation of Language Models](https://arxiv.org/abs/2111.02840)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/NeurIPS'21-f1b800)

# Paper

<img src="figure/map.png" alt="image" width="1000" height="auto" class="center">



## A. Safety


### A1. Jailbreak

-  [2024/01] **[How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs](https://arxiv.org/abs/2401.06373)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2024/01] **[MLLM-Protector: Ensuring MLLM's Safety Without Hurting Performance](https://arxiv.org/abs/2401.02906)** <a href="https://github.com/pipilurj/MLLM-protector"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/VLM-c7688b) ![img](https://img.shields.io/badge/Defense-87b800)
-  [2024/01] **[Intention Analysis Prompting Makes Large Language Models a Good Jailbreak Defender](https://arxiv.org/abs/2401.06561)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Defense-87b800)
-  [2023/12] **[A Mutation-Based Method for Multi-Modal Jailbreaking Attack Detection](https://arxiv.org/abs/2312.10766)** ![img](https://img.shields.io/badge/VLM-c7688b) ![img](https://img.shields.io/badge/Defense-87b800)
-  [2023/12] **[Adversarial Attacks on GPT-4 via Simple Random Search](https://www.andriushchenko.me/gpt4adv.pdf)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/12] **[Make Them Spill the Beans! Coercive Knowledge Extraction From (Production) LLMs](https://arxiv.org/abs/2312.04782)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/CodeGen-87b800)
-  [2023/12] **[Safety Alignment in NLP Tasks: Weakly Aligned Summarization as an in-Context Attack](https://arxiv.org/abs/2312.06924)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/11] **[Evil Geniuses: Delving Into the Safety of LLM-based Agents](https://arxiv.org/abs/2311.11855)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Agent-87b800)
-  [2023/11] **[FigStep: Jailbreaking Large Vision-Language Models via Typographic Visual Prompts](https://arxiv.org/abs/2311.05608)** <a href="https://github.com/ThuCCSLab/FigStep"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/VLM-c7688b)
-  [2023/11] **[Summon a Demon and Bind It: A Grounded Theory of LLM Red Teaming in the Wild](https://arxiv.org/abs/2311.06237)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/10] **[Adversarial Attacks on LLMs](https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Blog-f1b800)
-  [2023/10] **[AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models](https://arxiv.org/abs/2310.04451)** <a href="https://github.com/SheltonLiu-N/AutoDAN"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/10] **[AutoDAN: Interpretable Gradient-Based Adversarial Attacks on Large Language Models](https://arxiv.org/abs/2310.15140)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/10] **[Jailbreak and Guard Aligned Language Models With Only Few in-Context Demonstrations](https://arxiv.org/abs/2310.06387)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/10] **[Jailbreaking Black Box Large Language Models in Twenty Queries](https://arxiv.org/abs/2310.08419)** <a href="https://github.com/patrickrchao/JailbreakingLLMs"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/10] **[Low-Resource Languages Jailbreak GPT-4](https://arxiv.org/abs/2310.02446)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/10] **[SC-Safety: A Multi-Round Open-Ended Question Adversarial Safety Benchmark for Large Language Models in Chinese](https://arxiv.org/abs/2310.05818)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Chinese-87b800)
-  [2023/10] **[SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks](https://arxiv.org/abs/2310.03684)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Defense-87b800)
-  [2023/09] **[GPTFUZZER: Red Teaming Large Language Models With Auto-Generated Jailbreak Prompts](https://arxiv.org/abs/2309.10253)** <a href="https://github.com/sherdencooper/GPTFuzz"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/09] **[Open Sesame! Universal Black Box Jailbreaking of Large Language Models](https://arxiv.org/abs/2309.01446)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/09] **[Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM](https://arxiv.org/abs/2309.14348)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Defense-87b800)
-  [2023/09] **[FuzzLLM: A Novel and Universal Fuzzing Framework for Proactively Discovering Jailbreak Vulnerabilities in Large Language Models](https://arxiv.org/abs/2309.05274)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/09] **[Catastrophic Jailbreak of Open-Source LLMs via Exploiting Generation](https://openreview.net/forum?id=r42tSSCHPh)** <a href="https://github.com/Princeton-SysML/Jailbreak_LLM"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
-  [2023/09] **[Generating Stealthy Jailbreak Prompts on Aligned Large Language Models](https://openreview.net/forum?id=7Jwpw4qKkb)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[GPT-4 Is Too Smart to Be Safe: Stealthy Chat With LLMs via Cipher](https://openreview.net/forum?id=MbfAK4s61A)** <a href="https://github.com/RobustNLP/CipherChat"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Jailbreak in Pieces: Compositional Adversarial Attacks on Multi-Modal Language Models](https://openreview.net/forum?id=plmBsXHxgR)** ![img](https://img.shields.io/badge/VLM-c7688b) ![img](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
-  [2023/09] **[Multilingual Jailbreak Challenges in Large Language Models](https://openreview.net/forum?id=vESNKdEMGp)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[On the Humanity of Conversational AI: Evaluating the Psychological Portrayal of LLMs](https://openreview.net/forum?id=H3UayAQWoE)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24_(Oral)-f1b800)
-  [2023/09] **[Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models That Follow Instructions](https://openreview.net/forum?id=gT5hALch9z)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/defense-87b800) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Understanding Hidden Context in Preference Learning: Consequences for RLHF](https://openreview.net/forum?id=0tWTxYYPnW)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/08] **[“Do Anything Now”: Characterizing and Evaluating in-the-Wild Jailbreak Prompts on Large Language Models](https://arxiv.org/abs/2308.03825)** <a href="https://github.com/verazuo/jailbreak_llms"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/08] **[Detecting Language Model Attacks With Perplexity](https://arxiv.org/abs/2308.14132)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Defense-87b800)
-  [2023/08] **[XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models](https://arxiv.org/abs/2308.01263)** <a href="https://github.com/paul-rottger/exaggerated-safety"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Defense-87b800)
-  [2023/07] **[Jailbroken: How Does LLM Safety Training Fail?](https://arxiv.org/abs/2307.02483)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/NeurIPS'23-f1b800)
-  [2023/07] **[MasterKey: Automated Jailbreak Across Multiple Large Language Model Chatbots](https://arxiv.org/abs/2307.08715)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/NDSS'24-f1b800)
-  [2023/07] **[Universal and Transferable Adversarial Attacks on Aligned Language Models](https://arxiv.org/abs/2307.15043)** <a href="https://github.com/llm-attacks/llm-attacks"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/07] **[Latent Jailbreak: A Benchmark for Evaluating Text Safety and Output Robustness of Large Language Models](https://arxiv.org/abs/2307.08487)** <a href="qiuhuachuan/latent-jailbreak (github.com)"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/05] **[Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study](https://arxiv.org/abs/2305.13860)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/05] **[Tricking LLMs Into Disobedience: Understanding, Analyzing, and Preventing Jailbreaks](https://arxiv.org/abs/2305.14965)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/04] **[Multi-Step Jailbreaking Privacy Attacks on ChatGPT](https://arxiv.org/abs/2304.05197)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/EMNLP'23_(Findings)-f1b800)

### A2. Alignment

-  [2024/01] **[Agent Alignment in Evolving Social Norms](https://arxiv.org/abs/2401.04620)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Agent-87b800)
-  [2023/12] **[Exploiting Novel GPT-4 APIs](https://arxiv.org/abs/2312.14302)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/12] **[Alignment for Honesty](https://arxiv.org/abs/2312.07000)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/11] **[Removing RLHF Protections in GPT-4 via Fine-Tuning](https://arxiv.org/abs/2311.05553)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/10] **[Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models](https://arxiv.org/abs/2310.02949v1)** <a href="https://github.com/BeyonderXX/ShadowAlignment"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/10] **[AI Alignment: A Comprehensive Survey](https://arxiv.org/abs/2310.19852)** <a href="https://github.com/PKU-Alignment/AlignmentSurvey?tab=readme-ov-file"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/09] **[Alignment as Reward-Guided Search](https://openreview.net/forum?id=shgx0eqdw6)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Beyond Imitation: Leveraging Fine-Grained Quality Signals for Alignment](https://openreview.net/forum?id=LNLjU5C5dK)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Beyond Reverse KL: Generalizing Direct Preference Optimization With Diverse Divergence Constraints](https://openreview.net/forum?id=2cRzmWXK9N)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
-  [2023/09] **[CAS: A Probability-Based Approach for Universal Condition Alignment Score](https://openreview.net/forum?id=E78OaH2s3f)** ![img](https://img.shields.io/badge/Diffusion-a99cf4) ![img](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
-  [2023/09] **[CPPO: Continual Learning for Reinforcement Learning With Human Feedback](https://openreview.net/forum?id=86zAUE80pP)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Fine-Tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!](https://openreview.net/forum?id=hTEGyKf0dZ)** <a href="https://github.com/LLM-Tuning-Safety/LLMs-Finetuning-Safety"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24_(Oral)-f1b800)
-  [2023/09] **[FLASK: Fine-Grained Language Model Evaluation Based on Alignment Skill Sets](https://openreview.net/forum?id=CYmF38ysDa)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
-  [2023/09] **[Gaining Wisdom From Setbacks: Aligning Large Language Models via Mistake Analysis](https://openreview.net/forum?id=aA33A70IO6)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Generative Judge for Evaluating Alignment](https://openreview.net/forum?id=gtkFw6sZGS)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Group Preference Optimization: Few-Shot Alignment of Large Language Models](https://openreview.net/forum?id=DpFeMH4l8Q)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Improving Generalization of Alignment With Human Preferences Through Group Invariant Learning](https://openreview.net/forum?id=fwCoLe3TAX)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
-  [2023/09] **[Large Language Models as Automated Aligners for  Benchmarking  Vision-Language Models](https://openreview.net/forum?id=kZEXgtMNNo)** ![img](https://img.shields.io/badge/VLM-c7688b) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models](https://openreview.net/forum?id=dKl6lMwbCy)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[RAIN: Your Language Models Can Align Themselves Without Finetuning](https://openreview.net/forum?id=pETSfWMUzy)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[RLCD: Reinforcement Learning From Contrastive Distillation for LM Alignment](https://openreview.net/forum?id=v3XXtxWKi6)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Safe RLHF: Safe Reinforcement Learning From Human Feedback](https://openreview.net/forum?id=TyFrPOKYXw)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
-  [2023/09] **[SALMON: Self-Alignment With Principle-Following Reward Models](https://openreview.net/forum?id=xJbsmB8UMx)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Self-Alignment With Instruction Backtranslation](https://openreview.net/forum?id=1oijHJBRsT)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24_(Oral)-f1b800)
-  [2023/09] **[Statistical Rejection Sampling Improves Preference Optimization](https://openreview.net/forum?id=xbjSwwrQOe)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[True Knowledge Comes From Practice: Aligning Large Language Models With Embodied Environments via Reinforcement Learning](https://openreview.net/forum?id=hILVmJ4Uvu)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Urial: Aligning Untuned LLMs With Just the 'Write' Amount of in-Context Learning](https://openreview.net/forum?id=wxJ0eXwwda)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[What Happens When You Fine-Tuning Your Model? Mechanistic Analysis of Procedurally Generated Tasks.](https://openreview.net/forum?id=A0HKeKl4Nl)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning](https://openreview.net/forum?id=BTKAeLqLMw)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/07] **[BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset](https://arxiv.org/abs/2307.04657)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/07] **[CValues: Measuring the Values of Chinese Large Language Models From Safety to Responsibility](https://arxiv.org/abs/2307.09705)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Chinese-87b800)
-  [2023/05] **[Principle-Driven Self-Alignment of Language Models From Scratch With Minimal Human Supervision](https://arxiv.org/abs/2305.03047)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/04] **[Fundamental Limitations of Alignment in Large Language Models](https://arxiv.org/abs/2304.11082)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/04] **[RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment](https://arxiv.org/abs/2304.06767)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2022/10] **[Enabling Classifiers to Make Judgements Explicitly Aligned With Human Values](https://arxiv.org/abs/2210.07652)** ![img](https://img.shields.io/badge/LLM-589cf4)

### A3. Deepfake

-  [2024/01] **[Few-Shot Detection of Machine-Generated Text Using Style Representations](https://arxiv.org/abs/2401.06712)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2024/01] **[LLM-as-a-Coauthor: The Challenges of Detecting LLM-Human Mixcase](https://arxiv.org/abs/2401.05952)** <a href="https://github.com/Dongping-Chen/MixSet"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2024/01] **[Authorship Obfuscation in Multilingual Machine-Generated Text Detection](https://arxiv.org/abs/2401.07867)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Multilingual-87b800)
-  [2023/10] **[Harnessing the Power of ChatGPT in Fake News: An in-Depth Exploration in Generation, Detection and Explanation](https://arxiv.org/abs/2310.05046)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/09] **[Can LLM-Generated Misinformation Be Detected?](https://openreview.net/forum?id=ccxD4mtkTU)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Detecting Machine-Generated Texts by Multi-Population Aware Optimization for Maximum Mean Discrepancy](https://openreview.net/forum?id=3fEKavFsnv)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Robustness of AI-Image Detectors: Fundamental Limits and Practical Attacks](https://openreview.net/forum?id=dLoAdIKENc)** ![img](https://img.shields.io/badge/Diffusion-a99cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/05] **[Evading Watermark Based Detection of AI-Generated Content](https://arxiv.org/abs/2305.03807)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/CCS'23-f1b800)
-  [2023/05] **[On the Risk of Misinformation Pollution With Large Language Models](https://arxiv.org/abs/2305.13661)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/EMNLP'23_(Findings)-f1b800)
-  [2023/04] **[Synthetic Lies: Understanding AI-Generated Misinformation and Evaluating Algorithmic and Human Solutions](https://doi.org/10.1145/3544548.3581318)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/CHI'23-f1b800)
-  [2023/03] **[Can AI-Generated Text Be Reliably Detected?](https://arxiv.org/abs/2303.11156)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/03] **[MGTBench: Benchmarking Machine-Generated Text Detection](https://arxiv.org/abs/2303.14822)** <a href="https://github.com/xinleihe/MGTBench"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2022/12] **[CoCo: Coherence-Enhanced Machine-Generated Text Detection Under Data Limitation With Contrastive Learning](https://arxiv.org/abs/2212.10341)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2022/12] **[Discovering Language Model Behaviors With Model-Written Evaluations](https://arxiv.org/abs/2212.09251)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ACL'23_(Findings)-f1b800)
-  [2022/10] **[DE-FAKE: Detection and Attribution of Fake Images Generated by Text-to-Image Generation Models](https://arxiv.org/abs/2210.06998)** ![img](https://img.shields.io/badge/Diffusion-a99cf4) ![img](https://img.shields.io/badge/CCS'23-f1b800)

### A4. Ethics
-  [1023/12] **[Exploiting Cultural Biases via Homoglyphs in Text-to-Image Synthesis](https://arxiv.org/abs/2209.08891)** ![img](https://img.shields.io/badge/Diffusion-a99cf4) ![img](https://img.shields.io/badge/JAIR'23-f1b800)
-  [2023/12] **[Disentangling Perceptions of Offensiveness: Cultural and Moral Correlates](https://arxiv.org/abs/2312.06861)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/10] **[Unpacking the Ethical Value Alignment in Big Models](https://arxiv.org/abs/2310.17551)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/09] **[Denevil: Towards Deciphering and Navigating the Ethical Values of Large Language Models via Instruction Learning](https://openreview.net/forum?id=m3RRWWFaVe)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/05] **[From Text to MITRE Techniques: Exploring the Malicious Use of Large Language Models for Generating Cyber Attack Payloads](https://arxiv.org/abs/2305.15336)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/01] **[Exploring AI Ethics of ChatGPT: A Diagnostic Analysis](https://arxiv.org/abs/2301.12867)** ![img](https://img.shields.io/badge/LLM-589cf4)

### A5. Fairness

-  [2024/01] **[Beyond the Surface: A Global-Scale Analysis of Visual Stereotypes in Text-to-Image Generation](https://arxiv.org/abs/2401.06310)** ![img](https://img.shields.io/badge/Diffusion-a99cf4)
-  [2023/12] **[GPTBIAS: A Comprehensive Framework for Evaluating Bias in Large Language Models](https://arxiv.org/abs/2312.06315)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/11] **[Beyond Detection: Unveiling Fairness Vulnerabilities in Abusive Language Models](https://arxiv.org/abs/2311.09428)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/11] **[FFT: Towards Harmlessness Evaluation and Analysis for LLMs With Factuality, Fairness, Toxicity](https://arxiv.org/abs/2311.18580)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/11] **[ROBBIE: Robust Bias Evaluation of Large Generative Language Models](https://arxiv.org/abs/2311.18140)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/EMNLP'23-f1b800)
-  [2023/10] **[Im Not Racist But...: Discovering Bias in the Internal Knowledge of Large Language Models](https://arxiv.org/abs/2310.08780)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/10] **[Investigating the Fairness of Large Language Models for Predictions on Tabular Data](https://arxiv.org/abs/2310.14607)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/10] **[Kelly Is a Warm Person, Joseph Is a Role Model: Gender Biases in LLM-Generated Reference Letters](https://arxiv.org/abs/2310.09219)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/EMNLP'23_(Findings)-f1b800)
-  [2023/09] **[Bias and Fairness in Chatbots: An Overview](https://arxiv.org/abs/2309.08836)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/09] **[Bias and Fairness in Large Language Models: A Survey](https://arxiv.org/abs/2309.00770)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/09] **[People's Perceptions Toward Bias and Related Concepts in Large Language Models: A Systematic Review](https://arxiv.org/abs/2309.14504)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/09] **[Achieving Fairness in Multi-Agent MDP Using Reinforcement Learning](https://openreview.net/forum?id=yoVq2BGQdP)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Agent-87b800) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs](https://openreview.net/forum?id=kGteeZ18Ir)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[FairVLM: Mitigating Bias in Pre-Trained Vision-Language Models](https://openreview.net/forum?id=HXoq9EqR9e)** ![img](https://img.shields.io/badge/VLM-c7688b) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Finetuning Text-to-Image Diffusion Models for Fairness](https://openreview.net/forum?id=hnrB5YHoYu)** ![img](https://img.shields.io/badge/Diffusion-a99cf4) ![img](https://img.shields.io/badge/ICLR'24_(Oral)-f1b800)
-  [2023/09] **[The Devil Is in the Neurons: Interpreting and Mitigating Social Biases in Language Models](https://openreview.net/forum?id=SQGUDc9tC8)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/08] **[FairBench: A Four-Stage Automatic Framework for Detecting Stereotypes and Biases in Large Language Models](https://arxiv.org/abs/2308.10397)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/08] **[Gender Bias and Stereotypes in Large Language Models](https://arxiv.org/abs/2308.14921)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/CI'23-f1b800)
-  [2023/07] **[Queer People Are People First: Deconstructing Sexual Identity Stereotypes in Large Language Models](https://arxiv.org/abs/2307.00101)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/06] **[Knowledge of Cultural Moral Norms in Large Language Models](https://arxiv.org/abs/2306.01857)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ACL'23-f1b800)
-  [2023/06] **[WinoQueer: A Community-in-the-Loop Benchmark for Anti-Lgbtq+ Bias in Large Language Models](https://arxiv.org/abs/2306.15087)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ACL'23-f1b800)
-  [2023/05] **[BiasAsker: Measuring the Bias in Conversational AI System](https://arxiv.org/abs/2305.12434)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/FSE'23-f1b800)
-  [2023/05] **[Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation](https://arxiv.org/abs/2305.07609)** <a href="https://github.com/jizhi-zhang/FaiRLLM"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Recsys'23-f1b800)
-  [2023/05] **[Large Language Models Are Not Fair Evaluators](https://arxiv.org/abs/2305.17926)** <a href="https://github.com/i-Eval/FairEval"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/05] **[Uncovering and Quantifying Social Biases in Code Generation](https://arxiv.org/abs/2305.15377)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/codeGen-87b800)
-  [2022/09] **[Moral Mimicry: Large Language Models Produce Moral Rationalizations Tailored to Political Identity](https://arxiv.org/abs/2209.12106)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ACL'23_(student)-f1b800)
-  [2022/05] **[Auto-Debias: Debiasing Masked Language Models With Automated Biased Prompts](https://aclanthology.org/2022.acl-long.72/)** <a href="https://github.com/Irenehere/Auto-Debias"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Defense-87b800) ![img](https://img.shields.io/badge/ACL'22-f1b800)
-  [2022/03] **[Mitigating Gender Bias in Distilled Language Models via Counterfactual Role Reversal](https://arxiv.org/abs/2203.12574)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ACL'22_(Findings)-f1b800)
-  [2021/04] **[Mitigating Political Bias in Language Models Through Reinforced Calibration](https://arxiv.org/abs/2104.14795)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/AAAI'21-f1b800)
-  [2021/02] **[Bias Out-of-the-Box: An Empirical Analysis of Intersectional Occupational Biases in Popular Generative Language Models](https://arxiv.org/abs/2102.04130)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/NeurIPS'21-f1b800)
-  [2021/01] **[Persistent Anti-Muslim Bias in Large Language Models](https://arxiv.org/abs/2101.05783)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/AIES'21-f1b800)

### A6. Hallucination

-  [2024/01] **[Seven Failure Points When Engineering a Retrieval Augmented Generation System](https://arxiv.org/abs/2401.05856)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/RAG-87b800)
-  [2024/01] **[Relying on the Unreliable: The Impact of Language Models' Reluctance to Express Uncertainty](https://arxiv.org/abs/2401.06730)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2024/01] **[Model Editing Can Hurt General Abilities of Large Language Models](https://arxiv.org/abs/2401.04700)** <a href="https://github.com/JasonForJoy/Model-Editing-Hurt"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/12] **[The Earth Is Flat Because...: Investigating LLMs' Belief Towards Misinformation via Persuasive Conversation](https://arxiv.org/abs/2312.09085)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/12] **[DelucionQA: Detecting Hallucinations in Domain-Specific Question Answering](https://arxiv.org/abs/2312.05200)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/EMNLP'23_(Findings)-f1b800)
-  [2023/12] **[Improving Factual Error Correction by Learning to Inject Factual Errors](https://arxiv.org/abs/2312.07049)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/12] **[RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment From Fine-Grained Correctional Human Feedback](https://arxiv.org/abs/2312.00849)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/11] **[Calibrated Language Models Must Hallucinate](https://arxiv.org/abs/2311.14648)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/11] **[A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions](https://arxiv.org/abs/2311.05232)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/11] **[Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models](https://arxiv.org/abs/2311.09210)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/11] **[Deficiency of Large Language Models in Finance: An Empirical Examination of Hallucination](https://arxiv.org/abs/2311.15548)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/11] **[Enhancing Uncertainty-Based Hallucination Detection With Stronger Focus](https://arxiv.org/abs/2311.13230)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/EMNLP'23-f1b800)
-  [2023/11] **[Holistic Analysis of Hallucination in GPT-4V(ision): Bias and Interference Challenges](https://arxiv.org/abs/2311.03287)** ![img](https://img.shields.io/badge/VLM-c7688b)
-  [2023/11] **[Mitigating Large Language Model Hallucinations via Autonomous Knowledge Graph-Based Retrofitting](https://arxiv.org/abs/2311.13314)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/11] **[UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained Generation](https://arxiv.org/abs/2311.15296)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Chinese-87b800)
-  [2023/11] **[When Large Language Models Contradict Humans? Large Language Models' Sycophantic Behaviour](https://arxiv.org/abs/2311.09410)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/10] **[Explainable Claim Verification via Knowledge-Grounded Reasoning With Large Language Models](https://arxiv.org/abs/2310.05253)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/EMNLP'23_(Findings)-f1b800)
-  [2023/10] **[Survey on Factuality in Large Language Models: Knowledge, Retrieval and Domain-Specificity](https://arxiv.org/abs/2310.07521)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/09] **[Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models](https://arxiv.org/abs/2309.01219)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/09] **[Analyzing and Mitigating Object Hallucination in Large Vision-Language Models](https://openreview.net/forum?id=oZDJKTlOUe)** ![img](https://img.shields.io/badge/VLM-c7688b) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Beyond Task Performance: Evaluating and Reducing the Flaws of Large Multimodal Models With in-Context-Learning](https://openreview.net/forum?id=mMaQvkMzDi)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[BTR: Binary Token Representations for Efficient Retrieval Augmented Language Models](https://openreview.net/forum?id=3TO3TtnOFl)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/RAG-87b800) ![img](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
-  [2023/09] **[Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting Over Heterogeneous Sources](https://openreview.net/forum?id=cPgh4gWZlz)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding](https://openreview.net/forum?id=4L0xnS4GQM)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Compressing LLMs: The Truth Is Rarely Pure and Never Simple](https://openreview.net/forum?id=B9klVS7Ddk)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Conformal Language Modeling](https://openreview.net/forum?id=pzUhfQ74c5)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[CRITIC: Large Language Models Can Self-Correct With Tool-Interactive Critiquing](https://openreview.net/forum?id=Sx038qxjek)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Davidsonian Scene Graph: Improving Reliability in Fine-Grained Evaluation for Text-Image Generation](https://openreview.net/forum?id=ITq4ZRUT4a)** ![img](https://img.shields.io/badge/Diffusion-a99cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Do Large Language Models Know About Facts?](https://openreview.net/forum?id=9OevMUdods)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
-  [2023/09] **[DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models](https://openreview.net/forum?id=Th6NyL07na)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Ferret: Refer and Ground Anything Anywhere at Any Granularity](https://openreview.net/forum?id=2msbbX3ydD)** ![img](https://img.shields.io/badge/VLM-c7688b) ![img](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
-  [2023/09] **[Fine-Tuning Language Models for Factuality](https://openreview.net/forum?id=WPZ2yPag4K)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[INSIDE: LLMs' Internal States Retain the Power of Hallucination Detection](https://openreview.net/forum?id=Zj12nzlQbz)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Lightweight Language Model Calibration for Open-Ended Question Answering With Varied Answer Lengths](https://openreview.net/forum?id=jH67LHVOIO)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[MetaGPT: Meta Programming for Multi-Agent Collaborative Framework](https://openreview.net/forum?id=VtmBAGCN7o)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Agent-87b800) ![img](https://img.shields.io/badge/ICLR'24_(Oral)-f1b800)
-  [2023/09] **[Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning](https://openreview.net/forum?id=J44HfH4JCg)** ![img](https://img.shields.io/badge/VLM-c7688b) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[RAPPER: Reinforced Rationale-Prompted Paradigm for Natural Language Explanation in Visual Question Answering](https://openreview.net/forum?id=bshfchPM9H)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning](https://openreview.net/forum?id=ZGNWW7xZ6Q)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Self-Contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation](https://openreview.net/forum?id=EmQSOi1X2f)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Supervised Knowledge Makes Large Language Models Better in-Context Learners](https://openreview.net/forum?id=bAMPOUF227)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Teaching Language Models to Hallucinate Less With Synthetic Tasks](https://openreview.net/forum?id=xpw7V0P136)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Teaching Large Language Models to Self-Debug](https://openreview.net/forum?id=KuPixIqPiq)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[The Reasonableness Behind Unreasonable Translation Capability of Large Language Model](https://openreview.net/forum?id=3KDbIWT26J)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Think-on-Graph: Deep and Responsible Reasoning of Large Language Model on Knowledge Graph](https://openreview.net/forum?id=nnVO1PvbTv)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Unveiling and Manipulating Prompt Influence in Large Language Models](https://openreview.net/forum?id=ap1ByuwQrX)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/08] **[Simple Synthetic Data Reduces Sycophancy in Large Language Models](https://arxiv.org/abs/2308.03958)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/07] **[A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation](https://arxiv.org/abs/2307.03987)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/07] **[Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models](https://arxiv.org/abs/2307.01379)** <a href="https://github.com/jinhaoduan/shifting-attention-to-relevance"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/06] **[Explore, Establish, Exploit: Red Teaming Language Models From Scratch](https://arxiv.org/abs/2306.09442)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/06] **[Inference-Time Intervention: Eliciting Truthful Answers From a Language Model](https://arxiv.org/abs/2306.03341)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/05] **[Fact-Checking Complex Claims With Program-Guided Reasoning](https://arxiv.org/abs/2305.12744)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ACL'23-f1b800)
-  [2023/05] **[HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models](https://arxiv.org/abs/2305.11747)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/EMNLP'23-f1b800)
-  [2023/05] **[Improving Factuality and Reasoning in Language Models Through Multiagent Debate](https://arxiv.org/abs/2305.14325)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/05] **[Knowledge of Knowledge: Exploring Known-Unknowns Uncertainty With Large Language Models](https://arxiv.org/abs/2305.13712)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/05] **[Mitigating Language Model Hallucination With Interactive Question-Knowledge Alignment](https://arxiv.org/abs/2305.13669)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/05] **[Sources of Hallucination by Large Language Models on Inference Tasks](https://arxiv.org/abs/2305.14552)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/EMNLP'23_(Findings)-f1b800)
-  [2023/05] **[Trusting Your Evidence: Hallucinate Less With Context-Aware Decoding](https://arxiv.org/abs/2305.14739)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/04] **[In ChatGPT We Trust? Measuring and Characterizing the Reliability of ChatGPT](https://arxiv.org/abs/2304.08979)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/03] **[SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models](https://arxiv.org/abs/2303.08896)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/EMNLP'23-f1b800)
-  [2023/02] **[A Categorical Archive of ChatGPT Failures](https://arxiv.org/abs/2302.03494)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/02] **[Check Your Facts and Try Again: Improving Large Language Models With External Knowledge and Automated Feedback](https://arxiv.org/abs/2302.12813)** <a href="https://github.com/pengbaolin/LLM-Augmenter"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2022/02] **[Locating and Editing Factual Associations in GPT](https://arxiv.org/abs/2202.05262)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/NeurIPS'22-f1b800)
-  [2022/02] **[Survey of Hallucination in Natural Language Generation](https://arxiv.org/abs/2202.03629)** ![img](https://img.shields.io/badge/LLM-589cf4)

### A7. Toxicity

-  [2024/01] **[A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity](https://arxiv.org/abs/2401.01967)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2024/01] **[MetaHate: A Dataset for Unifying Efforts on Hate Speech Detection](https://arxiv.org/abs/2401.06526)** <a href="https://github.com/palomapiot/metahate/"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/12] **[Efficient Toxic Content Detection by Bootstrapping and Distilling Large Language Models](https://arxiv.org/abs/2312.08303)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/12] **[GTA: Gated Toxicity Avoidance for LM Performance Preservation](https://arxiv.org/abs/2312.06122)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/EMNLP'23_(Findings)-f1b800)
-  [2023/12] **[Llama Guard: LLM-based Input-Output Safeguard for Human-Ai Conversations](https://arxiv.org/abs/2312.06674)** <a href="https://github.com/facebookresearch/PurpleLlama/tree/main/Llama-Guard"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/11] **[Unveiling the Implicit Toxicity in Large Language Models](https://arxiv.org/abs/2311.17391)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/EMNLP'23-f1b800)
-  [2023/10] **[On the Proactive Generation of Unsafe Images From Text-to-Image Models Using Benign Prompts](https://arxiv.org/abs/2310.16613)** ![img](https://img.shields.io/badge/VLM-c7688b)
-  [2023/10] **[All Languages Matter: On the Multilingual Safety of Large Language Models](https://arxiv.org/abs/2310.00905)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/09] **[(InThe)WildChat: 570K ChatGPT Interaction Logs in the Wild](https://openreview.net/forum?id=Bl8u7ZRlbM)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/New_dataset-87b800) ![img](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
-  [2023/09] **[Controlled Text Generation via Language Model Arithmetic](https://openreview.net/forum?id=SLw9fp4yI6)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
-  [2023/09] **[Curiosity-Driven Red-Teaming for Large Language Models](https://openreview.net/forum?id=4KqkizXgXU)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[RealChat-1M: A Large-Scale Real-World LLM Conversation Dataset](https://openreview.net/forum?id=BOfDKxfwt0)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/New_dataset-87b800) ![img](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
-  [2023/09] **[Understanding Catastrophic Forgetting in Language Models via Implicit Inference](https://openreview.net/forum?id=VrHiF2hsrm)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Unmasking and Improving Data Credibility: A Study With Datasets for Training Harmless Language Models](https://openreview.net/forum?id=6bcAD6g688)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[What's in My Big Data?](https://openreview.net/forum?id=RvfPnOkPV4)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
-  [2023/08] **[You Only Prompt Once: On the Capabilities of Prompt Learning on Large Language Models to Tackle Toxic Content](https://arxiv.org/abs/2308.05596)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/S&P'24-f1b800)
-  [2023/08] **[Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs](https://arxiv.org/abs/2308.13387)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/05] **[Unsafe Diffusion: On the Generation of Unsafe Images and Hateful Memes From Text-to-Image Models](https://arxiv.org/abs/2305.13873)** ![img](https://img.shields.io/badge/Diffusion-a99cf4) ![img](https://img.shields.io/badge/CCS'23-f1b800)
-  [2023/05] **[Evaluating ChatGPT's Performance for Multilingual and Emoji-Based Hate Speech Detection](https://arxiv.org/abs/2305.13276)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/04] **[Toxicity in ChatGPT: Analyzing Persona-Assigned Language Models](https://arxiv.org/abs/2304.05335)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/EMNLP'23_(Findings)-f1b800)
-  [2023/02] **[Adding Instructions During Pretraining: Effective Way of Controlling Toxicity in Language Models](https://arxiv.org/abs/2302.07388)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/EACL'23-f1b800)
-  [2023/02] **[Is ChatGPT Better Than Human Annotators? Potential and Limitations of ChatGPT in Explaining Implicit Hate Speech](https://arxiv.org/abs/2302.07736)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/WWW'23_(Companion_Volume)-f1b800)
-  [2022/12] **[Constitutional AI: Harmlessness From AI Feedback](https://arxiv.org/abs/2212.08073)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2022/12] **[On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning](https://arxiv.org/abs/2212.08061)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ACL'23-f1b800)
-  [2022/10] **[Unified Detoxifying and Debiasing in Language Generation via Inference-Time Adaptive Optimization](https://arxiv.org/abs/2210.04492)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'23-f1b800)
-  [2022/05] **[Toxicity Detection With Generative Prompt-Based Inference](https://arxiv.org/abs/2205.12390)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2022/04] **[Training a Helpful and Harmless Assistant With Reinforcement Learning From Human Feedback](https://arxiv.org/abs/2204.05862)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2022/03] **[ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection](https://arxiv.org/abs/2203.09509)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ACL'22-f1b800)
-  [2020/09] **[RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models](https://arxiv.org/abs/2009.11462)** <a href="https://github.com/allenai/real-toxicity-prompts?tab=readme-ov-file"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/EMNLP'20_(Findings)-f1b800)

## B. Security


### B1. Adversarial Examples

-  [2024/01] **[INSTRUCTTA: Instruction-Tuned Targeted Attack for Large Vision-Language Models](https://arxiv.org/abs/2312.01886)** ![img](https://img.shields.io/badge/VLM-c7688b)
-  [2024/01] **[Adversarial Examples Are Misaligned in Diffusion Model Manifolds](https://arxiv.org/abs/2401.06637)** ![img](https://img.shields.io/badge/Diffusion-a99cf4)
-  [2024/01] **[Exploring Adversarial Attacks Against Latent Diffusion Model From the Perspective of Adversarial Transferability](https://arxiv.org/abs/2401.07087)** ![img](https://img.shields.io/badge/Diffusion-a99cf4)
-  [2023/12] **[Causality Analysis for Evaluating the Security of Large Language Models](https://arxiv.org/abs/2312.07876)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/12] **[Hijacking Context in Large Multi-Modal Models](https://arxiv.org/abs/2312.07553)** ![img](https://img.shields.io/badge/VLM-c7688b)
-  [2023/11] **[Can Protective Perturbation Safeguard Personal Data From Being Exploited by Stable Diffusion?](https://arxiv.org/abs/2312.00084)** ![img](https://img.shields.io/badge/Diffusion-a99cf4) ![img](https://img.shields.io/badge/Defense-87b800)
-  [2023/11] **[DiffAttack: Evasion Attacks Against Diffusion-Based Adversarial Purification](https://arxiv.org/abs/2311.16124)** <a href="https://github.com/kangmintong/DiffAttack"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/Diffusion-a99cf4) ![img](https://img.shields.io/badge/NeurIPS'23-f1b800)
-  [2023/11] **[How Many Unicorns Are in This Image? A Safety Evaluation Benchmark for Vision LLMs](https://arxiv.org/abs/2311.16101)** <a href="https://github.com/UCSC-VLAA/vllm-safety-benchmark"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/VLM-c7688b) ![img](https://img.shields.io/badge/Benchmark-87b800)
-  [2023/11] **[Unveiling Safety Vulnerabilities of Large Language Models](https://arxiv.org/abs/2311.04124)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/10] **[Misusing Tools in Large Language Models With Visual Adversarial Examples](https://arxiv.org/abs/2310.03185)** ![img](https://img.shields.io/badge/VLM-c7688b)
-  [2023/09] **[How Robust Is Google's Bard to Adversarial Image Attacks?](https://arxiv.org/abs/2309.11751)** <a href="https://github.com/thu-ml/Attack-Bard"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/VLM-c7688b)
-  [2023/09] **[Image Hijacks: Adversarial Images Can Control Generative Models at Runtime](https://arxiv.org/abs/2309.00236)** <a href="https://github.com/euanong/image-hijacks"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/VLM-c7688b)
-  [2023/09] **[An Image Is Worth 1000 Lies: Transferability of Adversarial Images Across Prompts on Vision-Language Models](https://openreview.net/forum?id=nc5GgFAvtk)** ![img](https://img.shields.io/badge/VLM-c7688b) ![img](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
-  [2023/09] **[An LLM Can Fool Itself: A Prompt-Based Adversarial Attack](https://openreview.net/forum?id=VVgGbB9TNV)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Language Model Detectors Are Easily Optimized Against](https://openreview.net/forum?id=4eJDMjYZZG)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Leveraging Optimization for Adaptive Attacks on Image Watermarks](https://openreview.net/forum?id=O9PArxKLe1)** ![img](https://img.shields.io/badge/Diffusion-a99cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Tensor Trust: Interpretable Prompt Injection Attacks From an Online Game](https://openreview.net/forum?id=fsW7wJGLBd)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/New_dataset-87b800) ![img](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
-  [2023/09] **[Training Socially Aligned Language Models on Simulated Social Interactions](https://openreview.net/forum?id=NddKiWtdUm)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Defense-87b800) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/08] **[Ceci N'est Pas Une Pomme: Adversarial Illusions in Multi-Modal Embeddings](https://arxiv.org/abs/2308.11804)** <a href="https://github.com/ebagdasa/adversarial_illusions"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/VLM-c7688b)
-  [2023/08] **[On the Adversarial Robustness of Multi-Modal Foundation Models](https://arxiv.org/abs/2308.10741)** ![img](https://img.shields.io/badge/VLM-c7688b) ![img](https://img.shields.io/badge/ICCV'23_(AROW_Workshop)-f1b800)
-  [2023/08] **[Robustness Over Time: Understanding Adversarial Examples' Effectiveness on Longitudinal Versions of Large Language Models](https://arxiv.org/abs/2308.07847)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/07] **[Certified Robustness for Large Language Models With Self-Denoising](https://arxiv.org/abs/2307.07171)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Defense-87b800)
-  [2023/06] **[Adversarial Examples in the Age of ChatGPT](http://spylab.ai/blog/chatbot-adversarial-examples/)** ![img](https://img.shields.io/badge/VLM-c7688b) ![img](https://img.shields.io/badge/Blog-f1b800)
-  [2023/06] **[Are Aligned Neural Networks Adversarially Aligned?](https://arxiv.org/abs/2306.15447)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/NeurIPS'23-f1b800)
-  [2023/06] **[PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts](https://arxiv.org/abs/2306.04528)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Benchmark-87b800)
-  [2023/06] **[Stable Diffusion Is Unstable](https://arxiv.org/abs/2306.02583)** ![img](https://img.shields.io/badge/Diffusion-a99cf4) ![img](https://img.shields.io/badge/NeurIPS'23-f1b800)
-  [2023/06] **[Unlearnable Examples for Diffusion Models: Protect Data From Unauthorized Exploitation](https://arxiv.org/abs/2306.01902)** <a href="https://github.com/ZhengyueZhao/EUDP"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/Diffusion-a99cf4)
-  [2023/06] **[Visual Adversarial Examples Jailbreak Large Language Models](https://arxiv.org/abs/2306.13213)** <a href="https://github.com/Unispac/Visual-Adversarial-Examples-Jailbreak-Large-Language-Models"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/VLM-c7688b)
-  [2023/05] **[On Evaluating Adversarial Robustness of Large Vision-Language Models](https://arxiv.org/abs/2305.16934)** <a href="https://github.com/yunqing-me/AttackVLM"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/VLM-c7688b) ![img](https://img.shields.io/badge/NeurIPS'23-f1b800)
-  [2023/05] **[Assessing Hidden Risks of LLMs: An Empirical Study on Robustness, Consistency, and Credibility](https://arxiv.org/abs/2305.10235)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/03] **[Anti-DreamBooth: Protecting Users From Personalized Text-to-Image Synthesis](https://arxiv.org/abs/2303.15433)** <a href="https://github.com/VinAIResearch/Anti-DreamBooth"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/Diffusion-a99cf4) ![img](https://img.shields.io/badge/ICCV'23-f1b800)
-  [2023/02] **[Adversarial Example Does Good: Preventing Painting Imitation From Diffusion Models via Adversarial Examples](https://arxiv.org/abs/2302.04578)** <a href="https://github.com/mist-project/mist"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/Diffusion-a99cf4) ![img](https://img.shields.io/badge/ICML'23-f1b800)
-  [2023/02] **[Raising the Cost of Malicious AI-Powered Image Editing](https://arxiv.org/abs/2302.06588)** <a href="https://github.com/MadryLab/photoguard"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/Diffusion-a99cf4) ![img](https://img.shields.io/badge/ICML'23-f1b800)
-  [2023/02] **[Large Language Models Can Be Easily Distracted by Irrelevant Context](https://arxiv.org/abs/2302.00093)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICML'23-f1b800)
-  [2023/02] **[Not What You've Signed Up For: Compromising Real-World LLM-Integrated Applications With Indirect Prompt Injection](https://arxiv.org/abs/2302.12173v2)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/AISec_'23-f1b800) ![img](https://img.shields.io/badge/Best_paper-ff0000)
-  [2023/02] **[On the Robustness of ChatGPT: An Adversarial and Out-of-Distribution Perspective](https://arxiv.org/abs/2302.12095)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/01] **[On Robustness of Prompt-Based Semantic Parsing With Large Pre-Trained Language Model: An Empirical Study on Codex](https://arxiv.org/abs/2301.12868)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/CodeGen-87b800) ![img](https://img.shields.io/badge/EACL'23-f1b800)
-  [2022/12] **[Understanding Zero-Shot Adversarial Robustness for Large-Scale Model](https://arxiv.org/abs/2212.07016)** <a href="https://github.com/cvlab-columbia/ZSRobust4FoundationModel"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/VLM-c7688b) ![img](https://img.shields.io/badge/ICLR'23-f1b800)

### B2. Poisoning 

-  [2024/01] **[Sleeper Agents: Training Deceptive LLMs That Persist Through Safety Training](https://arxiv.org/abs/2401.05566)** <a href="https://github.com/anthropics/sleeper-agents-paper"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2024/01] **[Universal Vulnerabilities in Large Language Models: In-Context Learning Backdoor Attacks](https://arxiv.org/abs/2401.05949)** <a href="https://github.com/shuaizhao95/ICLAttack"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/12] **[Leveraging Diffusion-Based Image Variations for Robust Training on Poisoned Data](https://arxiv.org/abs/2310.06372)** ![img](https://img.shields.io/badge/Diffusion-a99cf4) ![img](https://img.shields.io/badge/NeurIPS\_Workshop'23-f1b800)
-  [2023/12] **[Unleashing Cheapfakes Through Trojan Plugins of Large Language Models](https://arxiv.org/abs/2312.00374)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/12] **[Poisoned ChatGPT Finds Work for Idle Hands: Exploring Developers' Coding Practices With Insecure Suggestions From Poisoned AI Models](https://arxiv.org/abs/2312.06227)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/12] **[Stealthy and Persistent Unalignment on Large Language Models via Backdoor Injections](https://arxiv.org/abs/2312.00027)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/11] **[Test-Time Backdoor Mitigation for Black-Box Large Language Models With Defensive Demonstrations](https://arxiv.org/abs/2311.09763)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Defense-87b800)
-  [2023/10] **[Rickrolling the Artist: Injecting Backdoors into Text Encoders for Text-to-Image Synthesis](https://arxiv.org/abs/2211.02408)** ![img](https://img.shields.io/badge/Diffusion-a99cf4) ![img](https://img.shields.io/badge/ICCV'23-f1b800)
-  [2023/10] **[Composite Backdoor Attacks Against Large Language Models](https://arxiv.org/abs/2310.07676)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/10] **[Large Language Models Are Better Adversaries: Exploring Generative Clean-Label Backdoor Attacks Against Text Classifiers](https://arxiv.org/abs/2310.18603)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/EMNLP'23_(Findings)-f1b800)
-  [2023/10] **[PoisonPrompt: Backdoor Attack on Prompt-Based Large Language Models](https://arxiv.org/abs/2310.12439)** <a href="grasses/PoisonPrompt: Code for paper: PoisonPrompt: Backdoor Attack on Prompt-based Large Language Models, IEEE ICASSP 2024 (github.com)"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICASSP'24-f1b800)
-  [2023/10] **[Prompt Injection Attacks and Defenses in LLM-Integrated Applications](https://arxiv.org/abs/2310.12815)** <a href="liu00222/Open-Prompt-Injection: Prompt injection attacks and defenses in LLM-integrated applications (github.com)"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Defense-87b800)
-  [2023/10] **[Prompt-Specific Poisoning Attacks on Text-to-Image Generative Models](https://arxiv.org/abs/2310.13828)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/09] **[BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models](https://openreview.net/forum?id=c93SBwz1Ma)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[BadEdit: Backdooring Large Language Models by Model Editing](https://openreview.net/forum?id=duZANm2ABX)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Universal Jailbreak Backdoors From Poisoned Human Feedback](https://openreview.net/forum?id=GxCGsxiAaK)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/08] **[LMSanitator: Defending Prompt-Tuning Against Task-Agnostic Backdoors](https://arxiv.org/abs/2308.13904)** <a href="https://github.com/meng-wenlong/LMSanitator"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/NDSS'24-f1b800)
-  [2023/08] **[The Poison of Alignment](https://arxiv.org/abs/2308.13449)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/07] **[Backdooring Instruction-Tuned Large Language Models With Virtual Prompt Injection](https://arxiv.org/abs/2307.16888)** <a href="https://github.com/wegodev2/virtual-prompt-injection"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/06] **[On the Exploitability of Instruction Tuning](https://arxiv.org/abs/2306.17194)** <a href="https://github.com/azshue/AutoPoison"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/NeurIPS'23-f1b800)
-  [2023/06] **[Prompt Injection Attack Against LLM-integrated Applications](https://arxiv.org/abs/2306.05499)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/05] **[Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models](https://arxiv.org/abs/2305.14710)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/05] **[Poisoning Language Models During Instruction Tuning](https://arxiv.org/abs/2305.00944)** <a href="https://github.com/AlexWan0/Poisoning-Instruction-Tuned-Models"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICML'23-f1b800)

## C. Privacy


### C1. Contamination

-  [2023/09] **[Proving Test Set Contamination for Black-Box Language Models](https://openreview.net/forum?id=KS8mIvetg2)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24_(Oral)-f1b800)
-  [2023/09] **[Time Travel in LLMs: Tracing Data Contamination in Large Language Models](https://openreview.net/forum?id=2Rwq6c3tvr)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
-  [2023/09] **[To the Cutoff... And Beyond? A Longitudinal Perspective on LLM Data Contamination](https://openreview.net/forum?id=m2NVG4Htxs)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[DyVal: Graph-Informed Dynamic Evaluation of Large Language Models](https://openreview.net/forum?id=gjfOL9z5Xr)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)

### C2. Copyright

-  [2024/01] **[Generative AI Has a Visual Plagiarism Problem](https://spectrum.ieee.org/midjourney-copyright)** ![img](https://img.shields.io/badge/Diffusion-a99cf4) ![img](https://img.shields.io/badge/Blog-f1b800)
-  [2023/12] **[Mark My Words: Analyzing and Evaluating Language Model Watermarks](https://arxiv.org/abs/2312.00273)** <a href="https://github.com/wagner-group/MarkMyWords"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/11] **[A Robust Semantics-Based Watermark for Large Language Model Against Paraphrasing](https://arxiv.org/abs/2311.08721)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/11] **[Protecting Intellectual Property of Large Language Model-Based Code Generation APIs via Watermarks](https://dl.acm.org/doi/abs/10.1145/3576915.3623120)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/CodeGen-87b800) ![img](https://img.shields.io/badge/CCS'23-f1b800)
-  [2023/09] **[A Private Watermark for Large Language Models](https://openreview.net/forum?id=gMLQwKDY3N)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[A Semantic Invariant Robust Watermark for Large Language Models](https://openreview.net/forum?id=6p8lpe4MNf)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Provable Robust Watermarking for AI-Generated Text](https://openreview.net/forum?id=SsmT8aO45L)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[SILO Language Models: Isolating Legal Risk in a Nonparametric Datastore](https://openreview.net/forum?id=ruk0nyQPec)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
-  [2023/08] **[PromptCARE: Prompt Copyright Protection by Watermark Injection and Verification](https://arxiv.org/abs/2308.02816)** <a href="https://github.com/grasses/PromptCARE"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/S&P'24-f1b800)
-  [2023/06] **[Generative Watermarking Against Unauthorized Subject-Driven Image Synthesis](https://arxiv.org/abs/2306.07754)** ![img](https://img.shields.io/badge/Diffusion-a99cf4)
-  [2023/05] **[Tree-Ring Watermarks: Fingerprints for Diffusion Images That Are Invisible and Robust](https://arxiv.org/abs/2305.20030)** <a href="https://github.com/YuxinWenRick/tree-ring-watermark"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/Diffusion-a99cf4) ![img](https://img.shields.io/badge/NeurIPS'23-f1b800)
-  [2023/05] **[Watermarking Diffusion Model](https://arxiv.org/abs/2305.12502)** ![img](https://img.shields.io/badge/Diffusion-a99cf4)
-  [2023/03] **[A Recipe for Watermarking Diffusion Models](https://arxiv.org/abs/2303.10137)** <a href="https://github.com/yunqing-me/WatermarkDM"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/Diffusion-a99cf4)
-  [2023/02] **[Glaze: Protecting Artists From Style Mimicry by Text-to-Image Models](https://arxiv.org/abs/2302.04222)** <a href="https://glaze.cs.uchicago.edu/"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Security'23-f1b800) ![img](https://img.shields.io/badge/Best_paper-ff0000)
-  [2023/01] **[A Watermark for Large Language Models](https://arxiv.org/abs/2301.10226)** <a href="github.com/jwkirchenbauer/lm-watermarking"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICML'23-f1b800) ![img](https://img.shields.io/badge/Best_paper-ff0000)

### C3. Data Reconstruction

-  [2023/11] **[Language Model Inversion](https://arxiv.org/abs/2311.13647)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/11] **[Scalable Extraction of Training Data From (Production) Language Models](https://arxiv.org/abs/2311.17035)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/09] **[Intriguing Properties of Data Attribution on Diffusion Models](https://openreview.net/forum?id=vKViCoKGcB)** ![img](https://img.shields.io/badge/Diffusion-a99cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/01] **[Extracting Training Data From Diffusion Models](https://arxiv.org/abs/2301.13188)** ![img](https://img.shields.io/badge/Diffusion-a99cf4) ![img](https://img.shields.io/badge/Security'23-f1b800)
-  [2020/12] **[Extracting Training Data From Large Language Models](https://arxiv.org/abs/2012.07805)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Security'21-f1b800)

### C4. Extraction 

-  [2023/11] **[Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs Through a Global Scale Prompt Hacking Competition](https://arxiv.org/abs/2311.16119)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/09] **[Teach LLMs to Phish: Stealing Private Information From Language Models](https://openreview.net/forum?id=qo21ZlfNu6)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/07] **[Prompts Should Not Be Seen as Secrets: Systematically Measuring Prompt Extraction Attack Success](https://arxiv.org/abs/2307.06865)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/03] **[On Extracting Specialized Code Abilities From Large Language Models: A Feasibility Study](https://arxiv.org/abs/2303.03012)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICSE'24-f1b800)
-  [2023/03] **[Stealing the Decoding Algorithms of Language Models](https://arxiv.org/abs/2303.04729)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/CCS'23-f1b800) ![img](https://img.shields.io/badge/Best_paper-ff0000)
-  [2023/02] **[Prompt Stealing Attacks Against Text-to-Image Generation Models](https://arxiv.org/abs/2302.09923)** ![img](https://img.shields.io/badge/Diffusion-a99cf4)

### C5. Inference

-  [2023/12] **[Black-Box Membership Inference Attacks Against Fine-Tuned Diffusion Models](https://arxiv.org/abs/2312.08207)** ![img](https://img.shields.io/badge/Diffusion-a99cf4)
-  [2023/11] **[Practical Membership Inference Attacks Against Fine-Tuned Large Language Models via Self-Prompt Calibration](https://arxiv.org/abs/2311.06062)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/10] **[Last One Standing: A Comparative Analysis of Security and Privacy of Soft Prompt Tuning, LoRA, and in-Context Learning](https://arxiv.org/abs/2310.11397)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/10] **[User Inference Attacks on Large Language Models](https://arxiv.org/abs/2310.09266)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/09] **[Privacy Side Channels in Machine Learning Systems](https://arxiv.org/abs/2309.05610)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/09] **[An Efficient Membership Inference Attack for the Diffusion Model by Proximal Initialization](https://openreview.net/forum?id=rpH9FcCEV6)** ![img](https://img.shields.io/badge/Diffusion-a99cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Beyond Memorization: Violating Privacy via Inference With Large Language Models](https://openreview.net/forum?id=kmn0BhQk7p)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
-  [2023/09] **[Can LLMs Keep a Secret? Testing  Privacy  Implications of Language Models  via Contextual Integrity Theory](https://openreview.net/forum?id=gmg7t8b4s0)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
-  [2023/09] **[Identifying the Risks of LM Agents With an LM-Emulated Sandbox](https://openreview.net/forum?id=GEcwtMk1uA)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Agent-87b800) ![img](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
-  [2023/08] **[White-Box Membership Inference Attacks Against Diffusion Models](https://arxiv.org/abs/2308.06405)** ![img](https://img.shields.io/badge/Diffusion-a99cf4)
-  [2023/07] **[ProPILE: Probing Privacy Leakage in Large Language Models](https://arxiv.org/abs/2307.01881)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/NeurIPS'23-f1b800)
-  [2023/03] **[Class Attribute Inference Attacks: Inferring Sensitive Class Information by Diffusion-Based Attribute Manipulations](https://arxiv.org/abs/2303.09289)** ![img](https://img.shields.io/badge/Diffusion-a99cf4)
-  [2022/10] **[Membership Inference Attacks Against Text-to-Image Generation Models](https://arxiv.org/abs/2210.00968)** ![img](https://img.shields.io/badge/Diffusion-a99cf4)

### C6. Privacy-Preserving Computation

-  [2023/10] **[Locally Differentially Private Document Generation Using Zero Shot Prompting](https://arxiv.org/abs/2310.16111)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/EMNLP'23_(Findings)-f1b800)
-  [2023/09] **[DP-Forward: Fine-Tuning and Inference on Language Models With Differential Privacy in Forward Pass](https://arxiv.org/abs/2309.06746)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Defense-87b800) ![img](https://img.shields.io/badge/CCS'23-f1b800)
-  [2023/09] **[Differentially Private Synthetic Data via Foundation Model APIs 1: Images](https://openreview.net/forum?id=YEhQs8POIo)** ![img](https://img.shields.io/badge/Diffusion-a99cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[DP-OPT: Make Large Language Model Your Differentially-Private Prompt Engineer](https://openreview.net/forum?id=Ifz3IgsEPX)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
-  [2023/09] **[Enhancing Small Medical Learners With Privacy-Preserving Contextual Prompting](https://openreview.net/forum?id=ztpy1gsUpT)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Improving LoRA in Privacy-Preserving Federated Learning](https://openreview.net/forum?id=NLPzL6HWNl)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Privacy-Preserving in-Context Learning for Large Language Models](https://openreview.net/forum?id=x4OPJ7lHVU)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Privacy-Preserving in-Context Learning With Differentially Private Few-Shot Generation](https://openreview.net/forum?id=oZtt0pRnOl)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Privately Aligning Language Models With Reinforcement Learning](https://openreview.net/forum?id=3d0OmYTNui)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/08] **[SIGMA: Secure GPT Inference With Function Secret Sharing](https://eprint.iacr.org/2023/1269)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/07] **[CipherGPT: Secure Two-Party GPT Inference](https://eprint.iacr.org/2023/1147)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/05] **[Privacy-Preserving Prompt Tuning for Large Language Model Services](https://arxiv.org/abs/2305.06212)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/05] **[Privacy-Preserving Recommender Systems With Synthetic Query Generation Using Differentially Private Large Language Models](https://arxiv.org/abs/2305.05973)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2022/10] **[EW-Tune: A Framework for Privately Fine-Tuning Large Language Models With Differential Privacy](https://arxiv.org/abs/2210.15042)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICDM'22_(Workshops)-f1b800)

### C7. Unlearning
-  [2023/12] **[Defending Our Privacy With Backdoors](https://arxiv.org/abs/2310.08320)** ![img](https://img.shields.io/badge/NeurIPS\_Workshop'23-f1b800)
-  [2023/10] **[In-Context Unlearning: Language Models as Few Shot Unlearners](https://arxiv.org/abs/2310.07579)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/10] **[Large Language Model Unlearning](https://arxiv.org/abs/2310.10683)** <a href="https://github.com/kevinyaobytedance/llm_unlearn"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/10] **[Unlearn What You Want to Forget: Efficient Unlearning for LLMs](https://arxiv.org/abs/2310.20150)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/10] **[Who's Harry Potter? Approximate Unlearning in LLMs](https://arxiv.org/abs/2310.02238?s=08)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/09] **[Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks](https://openreview.net/forum?id=7erlRDoaV8)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
-  [2023/09] **[Detecting Pretraining Data From Large Language Models](https://openreview.net/forum?id=zWqr3MQuNs)** <a href="https://swj0419.github.io/detect-pretrain.github.io/"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[Ring-a-Bell! How Reliable Are Concept Removal Methods for Diffusion Models?](https://openreview.net/forum?id=lm7MRcsFiS)** ![img](https://img.shields.io/badge/Diffusion-a99cf4) ![img](https://img.shields.io/badge/Defense-87b800) ![img](https://img.shields.io/badge/ICLR'24-f1b800)
-  [2023/09] **[SalUn: Empowering Machine Unlearning via Gradient-Based Weight Saliency in Both Image Classification and Generation](https://openreview.net/forum?id=gn0mIhQGNM)** ![img](https://img.shields.io/badge/Diffusion-a99cf4) ![img](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
-  [2023/07] **[Right to Be Forgotten in the Era of Large Language Models: Implications, Challenges, and Solutions](https://arxiv.org/abs/2307.03941)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/03] **[Erasing Concepts From Diffusion Models](https://arxiv.org/abs/2303.07345)** ![img](https://img.shields.io/badge/Diffusion-a99cf4)

# Acknowledgement

- Organizers: [Tianshuo Cong](https://tianshuocong.github.io/), [Xinlei He](https://xinleihe.github.io/), [Zhengyu Zhao](https://zhengyuzhao.github.io/), [Yugeng Liu](https://liu.ai/)

- This project is inspired by [LLM Security](https://llmsecurity.net/), [Awesome LLM Security](https://github.com/corca-ai/awesome-llm-security), [LLM Security & Privacy](https://github.com/chawins/llm-sp),             [UR2-LLMs](https://github.com/jxzhangjhu/Awesome-LLM-Uncertainty-Reliability-Robustness), [PLMpapers](https://github.com/thunlp/PLMpapers), [EvaluationPapers4ChatGPT](https://github.com/THU-KEG/EvaluationPapers4ChatGPT)

<img src="figure/logo.png" alt="image" width="500" height="auto">
